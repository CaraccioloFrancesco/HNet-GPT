# HNet-GPT
# HNet-GPT: Structure-Aware Code Generation via Hierarchical Encoding and Transformer Decoding

**Exploratory Research** - Seeking community feedback and collaboration

## The questions
- Can hierarchical encoding improve code understanding?
- How do we best combine structural and sequential information?
- What are the limits of small-scale transformer training?

## Current Results (Preliminary)
- Small-scale experiments show promise
- Need validation on larger datasets
- Seeking collaboration for scaling up


## Limitations & Future Work

## Call for Collaboration
I am  open-sourcing everything to:
- Enable reproducibility
- Gather community feedback
- Validate results across different setups
- Explore further improvements

## Preliminary Findings
but to be rerunned
- **40.6%** improvement over Pure GPT-2
- **39.5%** improvement over Pure HNet
- Comprehensive evaluation on MBPP dataset

## Next Steps
- [ ] Scale to larger models --> more recent GPT
- [ ] Test on more datasets --> train on C/C++
- [ ] Optimize architecture
- [ ] Community feedback integration
