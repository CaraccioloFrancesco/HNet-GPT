{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0: Environment Setup & Dependencies #\n",
        "# Environment Setup & Dependencies\n",
        "\n",
        "**First, install required packages:**"
      ],
      "metadata": {
        "id": "kyJZLyap_TJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install datasets==2.16.1\n",
        "!pip install huggingface_hub --upgrade\n",
        "!pip install nltk\n",
        "!pip install transformers>=4.21.0\n",
        "!pip install radon\n",
        "\n",
        "# Verify installations\n",
        "import datasets\n",
        "import transformers\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")"
      ],
      "metadata": {
        "id": "71SsYGiI_jUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Introduction & Setup #\n",
        "# HNet-GPT: Structure-Aware Code Generation via Hierarchical Encoding and Transformer Decoding\n",
        "\n",
        "> **Preliminary Research** - Novel hybrid architecture combining hierarchical encoding with GPT-2 generation\n",
        ">\n",
        "> This notebook stats the discussion on the HNet-GPT architecture.\n",
        "\n",
        "## Research Overview\n",
        "This work introduces **HNet-GPT**, a novel hybrid architecture that combines:\n",
        "- **Hierarchical Encoding (HNet)**: Structure-aware code understanding\n",
        "- **Sequential Generation (GPT-2)**: Proven autoregressive text generation\n",
        "- **Adaptive Fusion**: Smart integration of both approaches\n",
        "\n",
        "## Key Results Preview\n",
        "- **40.6%** better than Pure GPT-2 (8.20 vs 13.80 perplexity)\n",
        "- **39.5%** better than Pure HNet (8.20 vs 13.56 perplexity)\n",
        "- Best performance across most code generation metrics\n",
        "\n",
        "## Architecture Comparison\n",
        "Here are evaluated three fundamental approaches:\n",
        "1. **Pure HNet**: End-to-end hierarchical transformer\n",
        "2. **HNet-GPT2 Hybrid**: The novel approach\n",
        "3. **Pure GPT-2**: Sequential transformer baseline\n",
        "\n",
        "## Current Limitations\n",
        "Explain here that the other coding generation metrics perform poorly, this due to a bad implementation of the checks or the limited training. This section needs more work\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hOVo5BEVsjqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QPZywcSG_Qsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.makedirs(\"/content/drive/MyDrive/hnet_gpt2_models\", exist_ok=True)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Model, GPT2Config, AutoTokenizer\n",
        "import time\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import ast\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import ast\n",
        "import sys\n",
        "from io import StringIO\n",
        "import contextlib\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import radon.complexity as radon_complexity\n",
        "from radon.visitors import ComplexityVisitor\n"
      ],
      "metadata": {
        "id": "8DedGFc0suGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Section 2: Core Utilities & Helper Functions #\n",
        " # Core Utilities & Helper Functions\n",
        "\n",
        "This section contains essential utility functions used across all models:\n",
        "- **Top-k/Top-p filtering**: Advanced text generation sampling\n",
        "- **Data loading utilities**: MBPP dataset integration\n",
        "- **Evaluation metrics**: BLEU, syntax validity, function completion"
      ],
      "metadata": {
        "id": "Y5k70HaFuZUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k/Top-p filtering function\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf'), min_tokens_to_keep=1):\n",
        "    \"\"\"\n",
        "    Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "    Args:\n",
        "        logits: logits distribution shape (batch_size, vocabulary size)\n",
        "        top_k: (int or float) If set to int > 0, only the top_k least likely tokens are kept per batch item.\n",
        "               If set to float (0.0 < top_k < 1.0), the number of tokens kept per batch item represents\n",
        "               the percentage of the vocabulary size (e.g. 0.2 means keep 20% of the vocabulary).\n",
        "        top_p: (float) If set to < 1.0, only the most likely tokens with probabilities that add up to >= top_p are\n",
        "               kept for generation.\n",
        "        filter_value: (float) value that will be used to fill filtered tokens.\n",
        "        min_tokens_to_keep: (int) Minimum number of tokens that cannot be filtered.\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        if not isinstance(top_k, int):\n",
        "            top_k = int(top_k * logits.shape[-1]) # Convert to absolute number of tokens\n",
        "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1)) # Clamp to (min_tokens_to_keep, vocab_size)\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        if min_tokens_to_keep > 1:\n",
        "            # Keep at least min_tokens_to_keep (set to false the first min_tokens_to_keep)\n",
        "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
        "        # Shift the indices to the right to keep the first token above the threshold\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation functions\n",
        "def check_syntax_validity(code_text):\n",
        "    \"\"\"Check if generated code is syntactically valid Python\"\"\"\n",
        "    try:\n",
        "        ast.parse(code_text)\n",
        "        return True\n",
        "    except SyntaxError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def evaluate_execution_accuracy(model, test_loader, tokenizer, device, num_samples=50):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        if isinstance(batch, tuple) and len(batch) == 2:\n",
        "            input_tensor, metadata = batch\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        input_tensor = input_tensor.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(\n",
        "                input_tensor,\n",
        "                max_new_tokens=128,\n",
        "                do_sample=False,\n",
        "            )\n",
        "\n",
        "        generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        match = re.search(r\"def\\s+\\w+\\(.*\\):(.|\\n)*\", generated)\n",
        "        if not match:\n",
        "            continue\n",
        "\n",
        "        func_code = match.group(0)\n",
        "\n",
        "        try:\n",
        "            exec_globals = {}\n",
        "            exec(func_code, exec_globals)\n",
        "            func = [v for k, v in exec_globals.items() if callable(v)][0]\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        test_cases = metadata.get(\"test_list\", [])\n",
        "        passed = 0\n",
        "\n",
        "        for test in test_cases:\n",
        "            try:\n",
        "                if eval(test, {}, {\"input\": func}):\n",
        "                    passed += 1\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        correct += (passed == len(test_cases))\n",
        "        total += 1\n",
        "\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TIER 1: FUNCTIONAL CORRECTNESS\n",
        "# ============================================================================\n",
        "\n",
        "def extract_function_from_code(code_text):\n",
        "    \"\"\"Extract the first function definition from generated code\"\"\"\n",
        "    try:\n",
        "        tree = ast.parse(code_text)\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.FunctionDef):\n",
        "                # Get the function name and recreate the function\n",
        "                func_code = ast.get_source_segment(code_text, node)\n",
        "                if func_code is None:\n",
        "                    # Fallback: extract function manually\n",
        "                    lines = code_text.split('\\n')\n",
        "                    func_lines = []\n",
        "                    in_function = False\n",
        "                    for line in lines:\n",
        "                        if line.strip().startswith('def '):\n",
        "                            in_function = True\n",
        "                        if in_function:\n",
        "                            func_lines.append(line)\n",
        "                            if line.strip() and not line.startswith(' ') and not line.startswith('\\t') and not line.strip().startswith('def '):\n",
        "                                break\n",
        "                    func_code = '\\n'.join(func_lines)\n",
        "                return func_code, node.name\n",
        "    except:\n",
        "        pass\n",
        "    return None, None\n",
        "\n",
        "def safe_execute_function(func_code, func_name, test_input):\n",
        "    \"\"\"Safely execute a function with given input\"\"\"\n",
        "    try:\n",
        "        # Create isolated execution environment\n",
        "        exec_globals = {'__builtins__': __builtins__}\n",
        "        exec_locals = {}\n",
        "\n",
        "        # Execute the function definition\n",
        "        exec(func_code, exec_globals, exec_locals)\n",
        "\n",
        "        # Get the function\n",
        "        if func_name not in exec_locals:\n",
        "            return None, \"Function not found\"\n",
        "\n",
        "        func = exec_locals[func_name]\n",
        "\n",
        "        # Capture stdout for functions that print\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = captured_output = StringIO()\n",
        "\n",
        "        try:\n",
        "            # Execute with test input\n",
        "            if isinstance(test_input, (list, tuple)):\n",
        "                result = func(*test_input)\n",
        "            else:\n",
        "                result = func(test_input)\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "        return result, None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "def evaluate_functional_correctness(model, test_loader, tokenizer, device, num_samples=50):\n",
        "    \"\"\"\n",
        "    Tier 1: Evaluate functional correctness with test cases\n",
        "    This is the MOST IMPORTANT metric for code generation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create test cases for common programming tasks\n",
        "    test_cases = [\n",
        "        {\n",
        "            'prompt': 'def sum_list(numbers):\\n    \"\"\"Return sum of numbers in list\"\"\"\\n    ',\n",
        "            'test_inputs': [([1, 2, 3, 4], 10), ([0], 0), ([], 0), ([5, -3, 2], 4)],\n",
        "            'function_name': 'sum_list'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def max_element(arr):\\n    \"\"\"Return maximum element in array\"\"\"\\n    ',\n",
        "            'test_inputs': [([1, 5, 3], 5), ([0], 0), ([-1, -5, -2], -1)],\n",
        "            'function_name': 'max_element'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def is_even(n):\\n    \"\"\"Check if number is even\"\"\"\\n    ',\n",
        "            'test_inputs': [(4, True), (3, False), (0, True), (-2, True)],\n",
        "            'function_name': 'is_even'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def reverse_string(s):\\n    \"\"\"Reverse a string\"\"\"\\n    ',\n",
        "            'test_inputs': [(\"hello\", \"olleh\"), (\"\", \"\"), (\"a\", \"a\"), (\"abc\", \"cba\")],\n",
        "            'function_name': 'reverse_string'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    total_correct = 0\n",
        "    total_attempted = 0\n",
        "    detailed_results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for test_case in test_cases:\n",
        "            if total_attempted >= num_samples:\n",
        "                break\n",
        "\n",
        "            prompt = test_case['prompt']\n",
        "            prompt_tokens = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "            # Generate completion\n",
        "            try:\n",
        "                generated = model.generate(\n",
        "                    prompt_tokens.input_ids,\n",
        "                    max_length=prompt_tokens.input_ids.size(1) + 100,\n",
        "                    temperature=0.3,  # Lower temperature for more deterministic code\n",
        "                    do_sample=True,\n",
        "                    top_k=50,\n",
        "                    top_p=0.9,\n",
        "                    repetition_penalty=1.1,\n",
        "                    eos_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "                # Decode generated code\n",
        "                generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "                completion = generated_text[len(prompt):]\n",
        "                full_function = prompt + completion\n",
        "\n",
        "                # Extract function\n",
        "                func_code, func_name = extract_function_from_code(full_function)\n",
        "\n",
        "                if func_code and func_name == test_case['function_name']:\n",
        "                    # Test against all test cases\n",
        "                    passed_tests = 0\n",
        "                    for test_input, expected_output in test_case['test_inputs']:\n",
        "                        result, error = safe_execute_function(func_code, func_name, test_input)\n",
        "                        if error is None and result == expected_output:\n",
        "                            passed_tests += 1\n",
        "\n",
        "                    success_rate = passed_tests / len(test_case['test_inputs'])\n",
        "                    if success_rate == 1.0:  # All tests passed\n",
        "                        total_correct += 1\n",
        "\n",
        "                    detailed_results.append({\n",
        "                        'prompt': prompt,\n",
        "                        'generated': completion[:100],\n",
        "                        'success_rate': success_rate,\n",
        "                        'passed_tests': passed_tests,\n",
        "                        'total_tests': len(test_case['test_inputs'])\n",
        "                    })\n",
        "\n",
        "                total_attempted += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Generation error: {e}\")\n",
        "                total_attempted += 1\n",
        "                continue\n",
        "\n",
        "    functional_accuracy = total_correct / total_attempted if total_attempted > 0 else 0.0\n",
        "\n",
        "    print(f\"\\n🎯 FUNCTIONAL CORRECTNESS EVALUATION:\")\n",
        "    print(f\"   Perfect Solutions: {total_correct}/{total_attempted} ({functional_accuracy:.1%})\")\n",
        "\n",
        "    return functional_accuracy, detailed_results\n",
        "\n",
        "# ============================================================================\n",
        "# TIER 2: CODE QUALITY METRICS\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_cyclomatic_complexity(code_text):\n",
        "    \"\"\"Calculate cyclomatic complexity of code\"\"\"\n",
        "    try:\n",
        "        tree = ast.parse(code_text)\n",
        "        visitor = ComplexityVisitor.from_ast(tree)\n",
        "        complexities = [block.complexity for block in visitor.blocks]\n",
        "        return np.mean(complexities) if complexities else 1.0\n",
        "    except:\n",
        "        return float('inf')  # Invalid code\n",
        "\n",
        "def check_pep8_basic(code_text):\n",
        "    \"\"\"Basic PEP8 style checks\"\"\"\n",
        "    score = 1.0\n",
        "    lines = code_text.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        # Check line length\n",
        "        if len(line) > 79:\n",
        "            score -= 0.1\n",
        "\n",
        "        # Check indentation (should be 4 spaces)\n",
        "        if line.startswith(' ') and not line.startswith('    '):\n",
        "            if line.lstrip() != line:  # Has indentation but not 4 spaces\n",
        "                score -= 0.1\n",
        "\n",
        "    return max(0.0, score)\n",
        "\n",
        "def calculate_readability_score(code_text):\n",
        "    \"\"\"Simple readability score based on various factors\"\"\"\n",
        "    try:\n",
        "        tree = ast.parse(code_text)\n",
        "\n",
        "        # Count different elements\n",
        "        total_nodes = len(list(ast.walk(tree)))\n",
        "        function_nodes = len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)])\n",
        "        variable_nodes = len([n for n in ast.walk(tree) if isinstance(n, ast.Name)])\n",
        "\n",
        "        # Simple heuristic: fewer nodes per function = more readable\n",
        "        if function_nodes > 0:\n",
        "            complexity_ratio = total_nodes / function_nodes\n",
        "            readability = max(0.0, 1.0 - (complexity_ratio - 10) / 50)  # Normalize\n",
        "        else:\n",
        "            readability = 0.5\n",
        "\n",
        "        return readability\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def evaluate_code_quality(model, test_loader, tokenizer, device, num_samples=100):\n",
        "    \"\"\"\n",
        "    Tier 2: Evaluate code quality metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    quality_metrics = {\n",
        "        'syntax_validity': [],\n",
        "        'complexity_scores': [],\n",
        "        'pep8_scores': [],\n",
        "        'readability_scores': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sample_count = 0\n",
        "        for input_ids, _ in test_loader:\n",
        "            if sample_count >= num_samples:\n",
        "                break\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Generate code completion\n",
        "            try:\n",
        "                generated = model.generate(\n",
        "                    input_ids[:, :int(input_ids.size(1) * 0.7)],  # Use 70% as prompt\n",
        "                    max_length=input_ids.size(1),\n",
        "                    temperature=0.6,\n",
        "                    do_sample=True,\n",
        "                    top_k=50,\n",
        "                    top_p=0.9,\n",
        "                    repetition_penalty=1.2,\n",
        "                    eos_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "                for gen in generated:\n",
        "                    code_text = tokenizer.decode(gen, skip_special_tokens=True)\n",
        "\n",
        "                    # 1. Syntax validity\n",
        "                    syntax_valid = check_syntax_validity(code_text)\n",
        "                    quality_metrics['syntax_validity'].append(float(syntax_valid))\n",
        "\n",
        "                    if syntax_valid:  # Only evaluate quality metrics for valid code\n",
        "                        # 2. Complexity\n",
        "                        complexity = calculate_cyclomatic_complexity(code_text)\n",
        "                        quality_metrics['complexity_scores'].append(min(complexity, 10))  # Cap at 10\n",
        "\n",
        "                        # 3. PEP8 compliance\n",
        "                        pep8_score = check_pep8_basic(code_text)\n",
        "                        quality_metrics['pep8_scores'].append(pep8_score)\n",
        "\n",
        "                        # 4. Readability\n",
        "                        readability = calculate_readability_score(code_text)\n",
        "                        quality_metrics['readability_scores'].append(readability)\n",
        "\n",
        "                    sample_count += 1\n",
        "                    if sample_count >= num_samples:\n",
        "                        break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Quality evaluation error: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Calculate averages\n",
        "    results = {}\n",
        "    for metric, values in quality_metrics.items():\n",
        "        results[metric] = np.mean(values) if values else 0.0\n",
        "\n",
        "    print(f\"\\nCODE QUALITY EVALUATION:\")\n",
        "    print(f\"   Syntax Validity: {results['syntax_validity']:.1%}\")\n",
        "    print(f\"   Avg Complexity: {results['complexity_scores']:.2f}\")\n",
        "    print(f\"   PEP8 Compliance: {results['pep8_scores']:.1%}\")\n",
        "    print(f\"   Readability Score: {results['readability_scores']:.1%}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# TIER 3: STRUCTURAL SIMILARITY (Including BLEU)\n",
        "# ============================================================================\n",
        "\n",
        "def compare_ast_structures(code1, code2):\n",
        "    \"\"\"Compare AST structures of two code snippets\"\"\"\n",
        "    try:\n",
        "        tree1 = ast.parse(code1)\n",
        "        tree2 = ast.parse(code2)\n",
        "\n",
        "        # Get node types\n",
        "        nodes1 = [type(node).__name__ for node in ast.walk(tree1)]\n",
        "        nodes2 = [type(node).__name__ for node in ast.walk(tree2)]\n",
        "\n",
        "        # Calculate Jaccard similarity\n",
        "        set1, set2 = set(nodes1), set(nodes2)\n",
        "        intersection = len(set1.intersection(set2))\n",
        "        union = len(set1.union(set2))\n",
        "\n",
        "        return intersection / union if union > 0 else 0.0\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def calculate_identifier_similarity(code1, code2):\n",
        "    \"\"\"Calculate similarity of variable/function names\"\"\"\n",
        "    try:\n",
        "        # Extract identifiers using regex\n",
        "        identifiers1 = set(re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code1))\n",
        "        identifiers2 = set(re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code2))\n",
        "\n",
        "        # Remove Python keywords\n",
        "        keywords = {'def', 'if', 'else', 'for', 'while', 'return', 'import', 'from', 'class'}\n",
        "        identifiers1 -= keywords\n",
        "        identifiers2 -= keywords\n",
        "\n",
        "        # Calculate Jaccard similarity\n",
        "        if len(identifiers1) == 0 and len(identifiers2) == 0:\n",
        "            return 1.0\n",
        "        union = len(identifiers1.union(identifiers2))\n",
        "        intersection = len(identifiers1.intersection(identifiers2))\n",
        "\n",
        "        return intersection / union if union > 0 else 0.0\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def evaluate_structural_similarity(model, test_loader, tokenizer, device, num_samples=50):\n",
        "    \"\"\"\n",
        "    Tier 3: Evaluate structural similarity including BLEU\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    similarity_metrics = {\n",
        "        'bleu_scores': [],\n",
        "        'ast_similarities': [],\n",
        "        'identifier_similarities': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sample_count = 0\n",
        "        for input_ids, labels in test_loader:\n",
        "            if sample_count >= num_samples:\n",
        "                break\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "            batch_size = input_ids.size(0)\n",
        "\n",
        "            for b in range(batch_size):\n",
        "                if sample_count >= num_samples:\n",
        "                    break\n",
        "\n",
        "                # Split into prompt and target\n",
        "                split_point = int(input_ids.size(1) * 0.6)\n",
        "                prompt = input_ids[b:b+1, :split_point]\n",
        "                target = input_ids[b, split_point:]\n",
        "                target_text = tokenizer.decode(target, skip_special_tokens=True).strip()\n",
        "\n",
        "                if len(target_text) > 20:  # Only evaluate substantial targets\n",
        "                    try:\n",
        "                        # Generate completion\n",
        "                        generated = model.generate(\n",
        "                            prompt,\n",
        "                            max_length=split_point + len(target),\n",
        "                            temperature=0.7,\n",
        "                            do_sample=True,\n",
        "                            top_k=50,\n",
        "                            top_p=0.9,\n",
        "                            repetition_penalty=1.2,\n",
        "                            eos_token_id=tokenizer.eos_token_id\n",
        "                        )\n",
        "\n",
        "                        generated_text = tokenizer.decode(\n",
        "                            generated[0, split_point:], skip_special_tokens=True\n",
        "                        ).strip()\n",
        "\n",
        "                        if generated_text:\n",
        "                            # 1. BLEU Score (with disclaimer)\n",
        "                            reference = [target_text.split()]\n",
        "                            candidate = generated_text.split()\n",
        "\n",
        "                            if len(candidate) > 0:\n",
        "                                smoothing = SmoothingFunction().method1\n",
        "                                bleu = sentence_bleu(reference, candidate, smoothing_function=smoothing)\n",
        "                                similarity_metrics['bleu_scores'].append(bleu)\n",
        "\n",
        "                            # 2. AST Similarity\n",
        "                            ast_sim = compare_ast_structures(target_text, generated_text)\n",
        "                            similarity_metrics['ast_similarities'].append(ast_sim)\n",
        "\n",
        "                            # 3. Identifier Similarity\n",
        "                            id_sim = calculate_identifier_similarity(target_text, generated_text)\n",
        "                            similarity_metrics['identifier_similarities'].append(id_sim)\n",
        "\n",
        "                            sample_count += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "    # Calculate averages\n",
        "    results = {}\n",
        "    for metric, values in similarity_metrics.items():\n",
        "        results[metric] = np.mean(values) if values else 0.0\n",
        "\n",
        "    print(f\"\\nSTRUCTURAL SIMILARITY EVALUATION:\")\n",
        "    print(f\"   BLEU Score: {results['bleu_scores']:.3f} (structural similarity only)\")\n",
        "    print(f\"   AST Similarity: {results['ast_similarities']:.3f}\")\n",
        "    print(f\"   Identifier Similarity: {results['identifier_similarities']:.3f}\")\n",
        "    print(f\"   Note: BLEU measures structural similarity, not functional correctness\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# COMPREHENSIVE EVALUATION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model_comprehensive_enhanced(model, test_loader, device, tokenizer, model_name=\"Model\"):\n",
        "    \"\"\"Enhanced comprehensive evaluation with proper metric hierarchy\"\"\"\n",
        "\n",
        "    print(f\"Computing perplexity...\")\n",
        "    perplexity = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    print(\"TIER 1: FUNCTIONAL CORRECTNESS\")\n",
        "    functional_accuracy, _ = evaluate_functional_correctness(\n",
        "        model, test_loader, tokenizer, device, num_samples=10  # Reduced for speed\n",
        "    )\n",
        "\n",
        "    print(\"TIER 2: CODE QUALITY\")\n",
        "    quality_results = evaluate_code_quality(\n",
        "        model, test_loader, tokenizer, device, num_samples=20  # Reduced for speed\n",
        "    )\n",
        "\n",
        "    print(\"TIER 3: STRUCTURAL SIMILARITY\")\n",
        "    similarity_results = evaluate_structural_similarity(\n",
        "        model, test_loader, tokenizer, device, num_samples=15  # Reduced for speed\n",
        "    )\n",
        "\n",
        "    # Combine results\n",
        "    comprehensive_results = {\n",
        "        'perplexity': perplexity,\n",
        "        'functional_accuracy': functional_accuracy,\n",
        "        'syntax_validity': quality_results['syntax_validity'],\n",
        "        'complexity_score': quality_results['complexity_scores'],\n",
        "        'pep8_compliance': quality_results['pep8_scores'],\n",
        "        'readability': quality_results['readability_scores'],\n",
        "        'bleu_score': similarity_results['bleu_scores'],\n",
        "        'ast_similarity': similarity_results['ast_similarities'],\n",
        "        'identifier_similarity': similarity_results['identifier_similarities'],\n",
        "    }\n",
        "\n",
        "    print(f\"\\nSUMMARY FOR {model_name}:\")\n",
        "    print(f\"Functional Accuracy: {functional_accuracy:.1%} (PRIMARY)\")\n",
        "    print(f\"Perplexity: {perplexity:.2f}\")\n",
        "    print(f\"Syntax Validity: {quality_results['syntax_validity']:.1%}\")\n",
        "    print(f\"BLEU (structural): {similarity_results['bleu_scores']:.3f}\")\n",
        "\n",
        "    return comprehensive_results"
      ],
      "metadata": {
        "id": "pt9uXj9quilU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# IMMEDIATE FIXES - Add these to your existing notebook after Section 2\n",
        "# ============================================================================\n",
        "\n",
        "# 1. CODE TOKEN IDENTIFICATION FUNCTION\n",
        "# Add this after your existing utility functions in Section 2\n",
        "\n",
        "def get_comprehensive_code_tokens(tokenizer):\n",
        "    \"\"\"Identify all code-relevant tokens in the vocabulary\"\"\"\n",
        "\n",
        "    print(\"Identifying code-relevant tokens...\")\n",
        "\n",
        "    # All possible code elements\n",
        "    code_elements = [\n",
        "        # Single characters\n",
        "        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
        "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "\n",
        "        # Operators and symbols\n",
        "        '+', '-', '*', '/', '%', '=', '==', '!=', '>', '<', '>=', '<=',\n",
        "        '(', ')', '[', ']', '{', '}', '.', ',', ':', ';', '_', '\"', \"'\",\n",
        "\n",
        "        # Whitespace\n",
        "        ' ', '\\n', '\\t',\n",
        "\n",
        "        # Keywords\n",
        "        'return', 'if', 'else', 'elif', 'for', 'while', 'def', 'class',\n",
        "        'True', 'False', 'None', 'and', 'or', 'not', 'in', 'is',\n",
        "\n",
        "        # Built-in functions\n",
        "        'len', 'str', 'int', 'float', 'bool', 'list', 'dict', 'set',\n",
        "        'range', 'sum', 'max', 'min', 'abs', 'round',\n",
        "        'print', 'input', 'open',\n",
        "\n",
        "        # Common method names\n",
        "        'upper', 'lower', 'strip', 'split', 'join', 'replace', 'append', 'remove',\n",
        "\n",
        "        # With spaces (GPT-2 tokenizer includes spaces)\n",
        "        ' +', ' -', ' *', ' /', ' %', ' =', ' ==', ' !=', ' >', ' <', ' >=', ' <=',\n",
        "        ' a', ' b', ' c', ' x', ' y', ' z', ' n', ' i', ' j', ' k',\n",
        "        ' return', ' if', ' else', ' True', ' False', ' None',\n",
        "        ' 0', ' 1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9',\n",
        "    ]\n",
        "\n",
        "    # Get token IDs\n",
        "    code_token_ids = set()\n",
        "    for element in code_elements:\n",
        "        try:\n",
        "            token_ids = tokenizer.encode(element, add_special_tokens=False)\n",
        "            code_token_ids.update(token_ids)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Add common combinations that work\n",
        "    common_combinations = [\n",
        "        'a +', 'b +', 'x *', 'n %', '== 0', '> 0', '< 0',\n",
        "        'return a', 'return x', 'return n', 'return True', 'return False',\n",
        "        'len(', 'str(', 'int(', 'abs(', 'x)', 'a)', 'b)', 'n)',\n",
        "    ]\n",
        "\n",
        "    for combo in common_combinations:\n",
        "        try:\n",
        "            token_ids = tokenizer.encode(combo, add_special_tokens=False)\n",
        "            code_token_ids.update(token_ids)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    code_token_list = sorted(list(code_token_ids))\n",
        "    print(f\"Identified {len(code_token_list)} code-relevant tokens\")\n",
        "\n",
        "    # Test coverage for common code\n",
        "    test_code = \"a + b\"\n",
        "    test_tokens = tokenizer.encode(test_code, add_special_tokens=False)\n",
        "    overlap = len(set(test_tokens) & set(code_token_list))\n",
        "    print(f\"   Test coverage 'a + b': {overlap}/{len(test_tokens)} tokens ({overlap/len(test_tokens):.1%})\")\n",
        "\n",
        "    return code_token_list\n",
        "\n",
        "# ============================================================================\n",
        "# 2. CODE-FOCUSED EVALUATION FUNCTION\n",
        "# Add this function to evaluate your existing models\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_with_code_focus(model, tokenizer, device, code_token_ids, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate existing models with code-focused approach\"\"\"\n",
        "\n",
        "    print(f\"\\nCODE-FOCUSED EVALUATION: {model_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test prompts for code completion\n",
        "    test_cases = [\n",
        "        {\n",
        "            'prompt': 'def add(a, b):\\n    return ',\n",
        "            'expected_patterns': ['a', 'b', '+', 'a + b'],\n",
        "            'description': 'Basic addition'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def is_even(n):\\n    return ',\n",
        "            'expected_patterns': ['n', '%', '2', '==', '0', 'n % 2 == 0'],\n",
        "            'description': 'Boolean logic'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def max_two(x, y):\\n    if x > y:\\n        return ',\n",
        "            'expected_patterns': ['x', 'y', 'if', 'else'],\n",
        "            'description': 'Conditional logic'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def square(n):\\n    return ',\n",
        "            'expected_patterns': ['n', '*', 'n * n'],\n",
        "            'description': 'Simple expression'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def is_positive(x):\\n    return ',\n",
        "            'expected_patterns': ['x', '>', '0', 'x > 0'],\n",
        "            'description': 'Comparison'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Results tracking\n",
        "    total_tests = len(test_cases)\n",
        "    syntax_successes = 0\n",
        "    pattern_successes = 0\n",
        "    code_token_successes = 0\n",
        "\n",
        "    code_token_ids_set = set(code_token_ids)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, test_case in enumerate(test_cases, 1):\n",
        "            print(f\"\\nTest {i}: {test_case['description']}\")\n",
        "            print(f\"   Prompt: {test_case['prompt'].strip()}\")\n",
        "\n",
        "            prompt_tokens = tokenizer(test_case['prompt'], return_tensors='pt').to(device)\n",
        "\n",
        "            # Try different generation strategies\n",
        "            best_completion = \"\"\n",
        "            best_score = 0\n",
        "\n",
        "            strategies = [\n",
        "                {\"temperature\": 0.1, \"do_sample\": False, \"name\": \"Greedy\"},\n",
        "                {\"temperature\": 0.3, \"do_sample\": True, \"top_k\": 20, \"name\": \"Conservative\"},\n",
        "                {\"temperature\": 0.7, \"do_sample\": True, \"top_k\": 50, \"name\": \"Creative\"}\n",
        "            ]\n",
        "\n",
        "            for strategy in strategies:\n",
        "                try:\n",
        "                    # Generate with existing model method\n",
        "                    if hasattr(model, 'generate'):\n",
        "                        generated = model.generate(\n",
        "                            prompt_tokens.input_ids,\n",
        "                            max_length=prompt_tokens.input_ids.size(1) + 15,\n",
        "                            repetition_penalty=1.1,\n",
        "                            eos_token_id=tokenizer.eos_token_id,\n",
        "                            pad_token_id=tokenizer.eos_token_id,\n",
        "                            **{k: v for k, v in strategy.items() if k != 'name'}\n",
        "                        )\n",
        "                    else:\n",
        "                        # Fallback for models without generate method\n",
        "                        continue\n",
        "\n",
        "                    full_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "                    completion = full_text[len(test_case['prompt']):].strip()\n",
        "\n",
        "                    if completion and len(completion) > len(best_completion):\n",
        "                        best_completion = completion\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            print(f\"   Generated: '{best_completion}'\")\n",
        "\n",
        "            if not best_completion:\n",
        "                print(f\"No generation produced\")\n",
        "                continue\n",
        "\n",
        "            # Test 1: Syntax validity\n",
        "            try:\n",
        "                full_code = test_case['prompt'] + best_completion\n",
        "                ast.parse(full_code)\n",
        "                print(f\"Valid syntax\")\n",
        "                syntax_successes += 1\n",
        "            except SyntaxError:\n",
        "                print(f\"Syntax error\")\n",
        "\n",
        "            # Test 2: Contains expected patterns\n",
        "            pattern_found = False\n",
        "            for pattern in test_case['expected_patterns']:\n",
        "                if pattern in best_completion:\n",
        "                    print(f\"Contains expected pattern: '{pattern}'\")\n",
        "                    pattern_found = True\n",
        "                    break\n",
        "\n",
        "            if pattern_found:\n",
        "                pattern_successes += 1\n",
        "            else:\n",
        "                print(f\"No expected patterns found\")\n",
        "\n",
        "            # Test 3: Uses code tokens\n",
        "            completion_tokens = tokenizer.encode(best_completion, add_special_tokens=False)\n",
        "            code_token_overlap = len(set(completion_tokens) & code_token_ids_set)\n",
        "            code_token_ratio = code_token_overlap / len(completion_tokens) if completion_tokens else 0\n",
        "\n",
        "            print(f\"Code token usage: {code_token_overlap}/{len(completion_tokens)} ({code_token_ratio:.1%})\")\n",
        "\n",
        "            if code_token_ratio >= 0.5:  # At least 50% code tokens\n",
        "                print(f\"Good code token usage\")\n",
        "                code_token_successes += 1\n",
        "            else:\n",
        "                print(f\"Low code token usage\")\n",
        "\n",
        "    # Calculate final scores\n",
        "    syntax_rate = syntax_successes / total_tests\n",
        "    pattern_rate = pattern_successes / total_tests\n",
        "    code_token_rate = code_token_successes / total_tests\n",
        "\n",
        "    # Composite score (weighted)\n",
        "    composite_score = (syntax_rate * 0.4 + pattern_rate * 0.4 + code_token_rate * 0.2)\n",
        "\n",
        "    print(f\"\\nCODE-FOCUSED RESULTS FOR {model_name}:\")\n",
        "    print(f\"Syntax Validity: {syntax_successes}/{total_tests} ({syntax_rate:.1%})\")\n",
        "    print(f\"Pattern Recognition: {pattern_successes}/{total_tests} ({pattern_rate:.1%})\")\n",
        "    print(f\"Code Token Usage: {code_token_successes}/{total_tests} ({code_token_rate:.1%})\")\n",
        "    print(f\"Composite Score: {composite_score:.1%}\")\n",
        "\n",
        "    return {\n",
        "        'syntax_rate': syntax_rate,\n",
        "        'pattern_rate': pattern_rate,\n",
        "        'code_token_rate': code_token_rate,\n",
        "        'composite_score': composite_score\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ENHANCED RESULTS COMPARISON\n",
        "# Add this to compare all your existing models\n",
        "# ============================================================================\n",
        "\n",
        "def show_enhanced_results():\n",
        "    \"\"\"Show enhanced evaluation of your existing trained models\"\"\"\n",
        "\n",
        "    print(\"ENHANCED EVALUATION OF EXISTING MODELS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Testing your trained models with code-focused evaluation\")\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Get code tokens\n",
        "    code_token_ids = get_comprehensive_code_tokens(tokenizer)\n",
        "\n",
        "    # Test your existing models\n",
        "    models_to_test = [\n",
        "        {\n",
        "            'name': 'Pure HNet',\n",
        "            'creator': lambda: create_pure_hnet_model(vocab_size, tokenizer),\n",
        "            'file': 'Pure_HNet.pt'\n",
        "        },\n",
        "        {\n",
        "            'name': 'HNet-GPT2-Hybrid',\n",
        "            'creator': lambda: create_hnet_gpt2_hybrid(vocab_size, tokenizer),\n",
        "            'file': 'HNet-GPT2-Hybrid.pt'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Pure GPT-2',\n",
        "            'creator': lambda: create_pure_gpt2_baseline(vocab_size),\n",
        "            'file': 'Pure_GPT-2.pt'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_config in models_to_test:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TESTING: {model_config['name']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Create model\n",
        "            model = model_config['creator']().to(device)\n",
        "\n",
        "            # Load existing weights\n",
        "            model_path = f\"/content/drive/MyDrive/hnet_gpt2_models/{model_config['file']}\"\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            print(f\"Loaded existing model weights\")\n",
        "\n",
        "            # Enhanced evaluation\n",
        "            enhanced_results = evaluate_with_code_focus(\n",
        "                model, tokenizer, device, code_token_ids, model_config['name']\n",
        "            )\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'name': model_config['name'],\n",
        "                'loaded': True,\n",
        "                **enhanced_results\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Clear GPU memory\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to test {model_config['name']}: {e}\")\n",
        "            results.append({\n",
        "                'name': model_config['name'],\n",
        "                'loaded': False,\n",
        "                'composite_score': 0.0\n",
        "            })\n",
        "\n",
        "    # Show final comparison\n",
        "    print(f\"\\nENHANCED RESULTS COMPARISON\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Model':<20} {'Syntax':<8} {'Patterns':<10} {'Tokens':<8} {'Overall':<8}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    # Sort by composite score\n",
        "    sorted_results = sorted([r for r in results if r['loaded']],\n",
        "                           key=lambda x: x['composite_score'], reverse=True)\n",
        "\n",
        "    for i, result in enumerate(sorted_results, 1):\n",
        "        emoji = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\"\n",
        "        print(f\"{emoji} {result['name']:<18} {result['syntax_rate']:<8.1%} \"\n",
        "              f\"{result['pattern_rate']:<10.1%} {result['code_token_rate']:<8.1%} \"\n",
        "              f\"{result['composite_score']:<8.1%}\")\n",
        "\n",
        "    if sorted_results:\n",
        "        winner = sorted_results[0]\n",
        "        print(f\"\\nBEST PERFORMING MODEL: {winner['name']}\")\n",
        "        print(f\"   Overall Score: {winner['composite_score']:.1%}\")\n",
        "\n",
        "        if winner['composite_score'] > 0.5:\n",
        "            print(f\"STRONG PERFORMANCE! Model shows good code generation capability\")\n",
        "        elif winner['composite_score'] > 0.25:\n",
        "            print(f\"MODERATE PERFORMANCE! Model shows some code awareness\")\n",
        "        else:\n",
        "            print(f\"LEARNING OPPORTUNITY! Model needs code-focused training\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# 4. SIMPLE EXECUTION - Add this at the end\n",
        "# ============================================================================\n",
        "\n",
        "# Run the enhanced evaluation\n",
        "print(\"RUNNING ENHANCED EVALUATION OF YOUR EXISTING MODELS...\")\n",
        "print(\"This will test your trained models with code-focused metrics!\")\n",
        "print()\n",
        "\n",
        "# Uncomment the line below to run the enhanced evaluation:\n",
        "# enhanced_results = show_enhanced_results()\n",
        "\n",
        "print(\"To run the enhanced evaluation, uncomment the last line!\")\n",
        "print(\"   This will show how well your existing models actually perform on code tasks!\")"
      ],
      "metadata": {
        "id": "3GSlQQw0qNfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Section 3: Data Loading & Preprocessing #\n",
        " This section handles the MBPP (Mostly Basic Python Programs) dataset:\n",
        "- **Primary**: CodeSearchNet Python dataset (4,000 train + 800 test examples)\n",
        "- **Fallback**: High-quality synthetic Python code patterns\n",
        "- **Format**: Task description + code solution pairs\n",
        "- **Preprocessing**: Tokenization with GPT-2 tokenizer"
      ],
      "metadata": {
        "id": "fZrZa1uUytoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to import datasets, but don't fail if it's not available\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    DATASETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DATASETS_AVAILABLE = False\n",
        "    print(\"Datasets library not available, will use synthetic data\")\n",
        "\n",
        "\n",
        "\n",
        "def create_mbpp_data_loaders(tokenizer, device, max_length=512, batch_size=4):\n",
        "    \"\"\"Create data loaders for MBPP dataset or synthetic code data\"\"\"\n",
        "\n",
        "    class CodeDataset(Dataset):\n",
        "        def __init__(self, split='train', max_length=512):\n",
        "            self.examples = []\n",
        "            data_loaded = False\n",
        "\n",
        "            # Approach 1: Try loading CodeSearchNet Python (now with correct field names!)\n",
        "            if DATASETS_AVAILABLE and not data_loaded:\n",
        "                try:\n",
        "                    print(f\"Attempting to load CodeSearchNet Python {split} split...\")\n",
        "                    dataset = load_dataset('code_search_net', 'python', split=split, trust_remote_code=True)\n",
        "\n",
        "                    count = 0\n",
        "                    max_examples = 4000 if split == 'train' else 800  # Double the data\n",
        "\n",
        "                    for item in tqdm(dataset, desc=f\"Processing CodeSearchNet {split} data\"):\n",
        "                        if count >= max_examples:\n",
        "                            break\n",
        "\n",
        "                        # Use the CORRECT field names from the debug output\n",
        "                        code = item.get('func_code_string', '').strip()\n",
        "                        func_name = item.get('func_name', '').strip()\n",
        "                        docstring = item.get('func_documentation_string', '').strip()\n",
        "\n",
        "                        # Basic filtering for valid Python functions\n",
        "                        if code and len(code) > 30 and 'def ' in code:\n",
        "                            # Create a task-like format\n",
        "                            if docstring and len(docstring) > 5:\n",
        "                                text = f\"# Task: {docstring}\\n\\n# Solution:\\n{code}\"\n",
        "                            else:\n",
        "                                text = f\"# Function: {func_name}\\n\\n# Solution:\\n{code}\"\n",
        "\n",
        "                            # Tokenize\n",
        "                            tokens = tokenizer(\n",
        "                                text,\n",
        "                                truncation=True,\n",
        "                                max_length=max_length,\n",
        "                                padding='max_length',\n",
        "                                return_tensors='pt'\n",
        "                            )\n",
        "\n",
        "                            self.examples.append({\n",
        "                                'input_ids': tokens['input_ids'].squeeze(0),\n",
        "                                'labels': tokens['input_ids'].squeeze(0).clone()\n",
        "                            })\n",
        "                            count += 1\n",
        "\n",
        "                    if len(self.examples) > 0:\n",
        "                        data_loaded = True\n",
        "                        print(f\"Successfully loaded {len(self.examples)} CodeSearchNet examples for {split}\")\n",
        "                    else:\n",
        "                        print(f\"No valid examples found in CodeSearchNet {split}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load CodeSearchNet: {e}\")\n",
        "\n",
        "            # Approach 2: Create high-quality synthetic Python code data\n",
        "\n",
        "\n",
        "            ###\n",
        "            if not data_loaded:\n",
        "                print(f\"Creating synthetic Python code data for {split}...\")\n",
        "                num_examples = 500 if split == 'train' else 100\n",
        "\n",
        "                # More diverse and realistic code patterns\n",
        "                code_patterns = [\n",
        "                    # Function definitions\n",
        "                    \"def calculate_sum(numbers):\\n    '''Calculate sum of a list'''\\n    total = 0\\n    for num in numbers:\\n        total += num\\n    return total\",\n",
        "\n",
        "                    # Classes\n",
        "                    \"class DataProcessor:\\n    def __init__(self, data):\\n        self.data = data\\n        self.processed = False\\n    \\n    def process(self):\\n        self.data = [x * 2 for x in self.data]\\n        self.processed = True\",\n",
        "\n",
        "                    # List comprehensions and algorithms\n",
        "                    \"def find_primes(n):\\n    '''Find all prime numbers up to n'''\\n    primes = []\\n    for num in range(2, n + 1):\\n        is_prime = True\\n        for i in range(2, int(num ** 0.5) + 1):\\n            if num % i == 0:\\n                is_prime = False\\n                break\\n        if is_prime:\\n            primes.append(num)\\n    return primes\",\n",
        "\n",
        "                    # Recursion\n",
        "                    \"def fibonacci(n):\\n    '''Calculate nth Fibonacci number'''\\n    if n <= 1:\\n        return n\\n    return fibonacci(n - 1) + fibonacci(n - 2)\",\n",
        "\n",
        "                    # String manipulation\n",
        "                    \"def reverse_words(sentence):\\n    '''Reverse words in a sentence'''\\n    words = sentence.split()\\n    reversed_words = words[::-1]\\n    return ' '.join(reversed_words)\",\n",
        "\n",
        "                    # Dictionary operations\n",
        "                    \"def count_frequencies(items):\\n    '''Count frequency of each item'''\\n    freq_dict = {}\\n    for item in items:\\n        if item in freq_dict:\\n            freq_dict[item] += 1\\n        else:\\n            freq_dict[item] = 1\\n    return freq_dict\",\n",
        "\n",
        "                    # Error handling\n",
        "                    \"def safe_divide(a, b):\\n    '''Safely divide two numbers'''\\n    try:\\n        result = a / b\\n        return result\\n    except ZeroDivisionError:\\n        return None\\n    except TypeError:\\n        return 'Invalid input types'\",\n",
        "\n",
        "                    # Sorting algorithms\n",
        "                    \"def bubble_sort(arr):\\n    '''Implement bubble sort'''\\n    n = len(arr)\\n    for i in range(n):\\n        for j in range(0, n - i - 1):\\n            if arr[j] > arr[j + 1]:\\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\\n    return arr\",\n",
        "                ]\n",
        "\n",
        "                for i in range(num_examples):\n",
        "                    # Rotate through patterns and add variations\n",
        "                    base_code = code_patterns[i % len(code_patterns)]\n",
        "\n",
        "                    # Add task description\n",
        "                    task_descriptions = [\n",
        "                        \"Write a function to solve this problem\",\n",
        "                        \"Implement a solution for the following\",\n",
        "                        \"Create a Python function that handles this task\",\n",
        "                        \"Develop an algorithm to compute the result\",\n",
        "                    ]\n",
        "\n",
        "                    task = task_descriptions[i % len(task_descriptions)]\n",
        "                    full_text = f\"# Task: {task}\\n\\n{base_code}\"\n",
        "\n",
        "                    # Tokenize\n",
        "                    tokens = tokenizer(\n",
        "                        full_text,\n",
        "                        truncation=True,\n",
        "                        max_length=max_length,\n",
        "                        padding='max_length',\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "\n",
        "                    self.examples.append({\n",
        "                        'input_ids': tokens['input_ids'].squeeze(0),\n",
        "                        'labels': tokens['input_ids'].squeeze(0).clone()\n",
        "                    })\n",
        "\n",
        "                print(f\"Created {len(self.examples)} synthetic examples for {split}\")\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.examples)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.examples[idx]['input_ids'], self.examples[idx]['labels']\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\nPreparing datasets...\")\n",
        "    train_dataset = CodeDataset('train', max_length)\n",
        "    test_dataset = CodeDataset('test', max_length)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Avoid multiprocessing issues\n",
        "        pin_memory=True if device == 'cuda' else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True if device == 'cuda' else False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"   Training examples: {len(train_dataset)}\")\n",
        "    print(f\"   Test examples: {len(test_dataset)}\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   Max sequence length: {max_length}\")\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "3aM7aUrd6zft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Model Architectures"
      ],
      "metadata": {
        "id": "CTw1_1R27ZuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsection 4a: Pure GPT-2 Baseline\n",
        "##Pure GPT-2 Baseline Model\n",
        "\n",
        "Standard sequential transformer architecture:\n",
        "- **12-layer GPT-2** configuration\n",
        "- **Sequential processing** of code tokens\n",
        "- **No hierarchical structure**\n",
        "- **Baseline performance** for comparison"
      ],
      "metadata": {
        "id": "g2cSrXlU7eTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pure_gpt2_baseline(vocab_size):\n",
        "    \"\"\"Pure GPT-2 baseline for comparison\"\"\"\n",
        "\n",
        "    class PureGPT2(nn.Module):\n",
        "        def __init__(self, vocab_size, embed_dim=768):\n",
        "            super().__init__()\n",
        "\n",
        "            # Create GPT-2 configuration\n",
        "            self.config = GPT2Config(\n",
        "                vocab_size=vocab_size,\n",
        "                n_embd=embed_dim,\n",
        "                n_layer=12,\n",
        "                n_head=12,\n",
        "                activation_function='gelu_new',\n",
        "                resid_pdrop=0.1,\n",
        "                embd_pdrop=0.1,\n",
        "                attn_pdrop=0.1,\n",
        "            )\n",
        "\n",
        "            # Create GPT-2 model\n",
        "            self.gpt2 = GPT2Model(self.config)\n",
        "\n",
        "            # Output projection\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            # Forward through GPT-2\n",
        "            outputs = self.gpt2(input_ids)\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "\n",
        "            # Project to vocabulary\n",
        "            logits = self.output(hidden_states)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "            ##\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"FIXED: Use parameter instead of tokenizer reference\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "            eos_token_id = kwargs.get('eos_token_id', 50256)  # FIXED: Get EOS token from kwargs\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # FIXED: Use eos_token_id parameter instead of tokenizer\n",
        "                    if eos_token_id is not None and (next_token == eos_token_id).any():\n",
        "                        break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "\n",
        "    return PureGPT2(vocab_size)"
      ],
      "metadata": {
        "id": "BZQE3JXf7k2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsection 4b: Pure HNet Model\n",
        "##Pure HNet: End-to-End Hierarchical Architecture\n",
        "\n",
        "Novel hierarchical transformer with 4-level processing:\n",
        "1. **Chunk-Level Processing**: Fixed overlapping windows with attention pooling\n",
        "2. **Global Context**: Inter-chunk relationship modeling  \n",
        "3. **Hierarchical-Sequential Bridge**: Project back to sequence space\n",
        "4. **Final Sequence Processing**: Custom transformer layers\n",
        "\n",
        "**Key Innovations:**\n",
        "- Attention-based chunk pooling (vs mean pooling)\n",
        "- Explicit global context stage\n",
        "- End-to-end hierarchical optimization"
      ],
      "metadata": {
        "id": "BbiHLkkv74gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pure_hnet_model(vocab_size, tokenizer):\n",
        "    \"\"\"\n",
        "    Create Pure HNet model - End-to-end hierarchical transformer\n",
        "\n",
        "    Architecture Philosophy:\n",
        "    - Multi-level hierarchical processing (chunk → global → sequence)\n",
        "    - Attention-based chunk pooling for better representations\n",
        "    - Gated fusion between hierarchical and sequential features\n",
        "    - Designed specifically for structured text like code\n",
        "    \"\"\"\n",
        "\n",
        "    class PureHNetModel(nn.Module):\n",
        "        def __init__(self, vocab_size, tokenizer, embed_dim=768):\n",
        "            super().__init__()\n",
        "            self.tokenizer = tokenizer\n",
        "            self.embed_dim = embed_dim\n",
        "\n",
        "            print(f\"Building Pure HNet Architecture:\")\n",
        "            print(f\"Vocabulary: {vocab_size:,} tokens\")\n",
        "            print(f\"Embedding dimension: {embed_dim}\")\n",
        "\n",
        "            # ============= Core Embeddings =============\n",
        "            self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "            self.pos_embed = nn.Embedding(1024, embed_dim)\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "            # ============= Hierarchical Parameters =============\n",
        "            self.num_chunks = 32  # Number of hierarchical chunks\n",
        "            self.chunk_size = 512 // self.num_chunks\n",
        "            self.chunk_overlap = int(self.chunk_size * 0.25)  # 25% overlap\n",
        "\n",
        "            print(f\"Chunking strategy: {self.num_chunks} chunks of size {self.chunk_size}\")\n",
        "            print(f\"Chunk overlap: {self.chunk_overlap} tokens\")\n",
        "\n",
        "            # ============= Level 1: Chunk-Level Processing =============\n",
        "            chunk_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.chunk_encoder = nn.TransformerEncoder(chunk_encoder_layer, num_layers=6)\n",
        "\n",
        "            # ============= Level 2: Global Context Processing =============\n",
        "            global_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.global_encoder = nn.TransformerEncoder(global_encoder_layer, num_layers=4)\n",
        "\n",
        "            # ============= Level 3: Hierarchical-to-Sequential Bridge =============\n",
        "            self.chunk_projection = nn.Sequential(\n",
        "                nn.Linear(embed_dim, embed_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(embed_dim, embed_dim)\n",
        "            )\n",
        "\n",
        "            # Gating mechanism for hierarchical fusion\n",
        "            self.hierarchical_gate = nn.Sequential(\n",
        "                nn.Linear(embed_dim * 2, embed_dim),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            # ============= Level 4: Final Sequence Processing =============\n",
        "            sequence_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.sequence_encoder = nn.TransformerEncoder(sequence_encoder_layer, num_layers=6)\n",
        "\n",
        "            # ============= Output Layer =============\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "            # Initialize weights\n",
        "            self.apply(self._init_weights)\n",
        "\n",
        "            total_params = sum(p.numel() for p in self.parameters())\n",
        "            print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "        def _init_weights(self, module):\n",
        "            \"\"\"Initialize model weights with small random values\"\"\"\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "        def _create_hierarchical_chunks(self, hidden_states, input_ids):\n",
        "            \"\"\"\n",
        "            Create hierarchical chunks with smart overlapping\n",
        "\n",
        "            Key Innovation: Attention-based pooling instead of mean pooling\n",
        "            for better chunk representations\n",
        "            \"\"\"\n",
        "            B, L, D = hidden_states.shape\n",
        "            chunks = []\n",
        "\n",
        "            for batch_idx in range(B):\n",
        "                batch_chunks = []\n",
        "\n",
        "                # Create overlapping chunks for better context\n",
        "                for i in range(self.num_chunks):\n",
        "                    start = i * self.chunk_size\n",
        "                    end = min(start + self.chunk_size + self.chunk_overlap, L)\n",
        "\n",
        "                    if start < L:\n",
        "                        chunk_tokens = hidden_states[batch_idx, start:end]\n",
        "                        # Use attention pooling for better representations\n",
        "                        chunk_repr = self._attention_pool(chunk_tokens)\n",
        "                        batch_chunks.append(chunk_repr)\n",
        "\n",
        "                # Ensure consistent chunk count\n",
        "                while len(batch_chunks) < self.num_chunks:\n",
        "                    batch_chunks.append(torch.zeros(D, device=hidden_states.device))\n",
        "\n",
        "                batch_chunks = batch_chunks[:self.num_chunks]\n",
        "                chunks.append(torch.stack(batch_chunks))\n",
        "\n",
        "            return torch.stack(chunks, dim=0)\n",
        "\n",
        "        def _attention_pool(self, chunk_tokens):\n",
        "            \"\"\"\n",
        "            Attention-based pooling for chunk representation\n",
        "\n",
        "            Better than mean pooling as it focuses on important tokens\n",
        "            \"\"\"\n",
        "            if chunk_tokens.size(0) == 0:\n",
        "                return torch.zeros(self.embed_dim, device=chunk_tokens.device)\n",
        "\n",
        "            # Compute attention weights based on token importance\n",
        "            chunk_mean = chunk_tokens.mean(dim=0, keepdim=True)\n",
        "            attention_scores = torch.sum(chunk_tokens * chunk_mean, dim=-1)\n",
        "            attention_weights = torch.softmax(attention_scores, dim=0)\n",
        "\n",
        "            # Weighted sum using attention\n",
        "            return torch.sum(chunk_tokens * attention_weights.unsqueeze(-1), dim=0)\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            \"\"\"\n",
        "            Four-level hierarchical forward pass:\n",
        "            1. Embeddings + Chunking\n",
        "            2. Chunk-level processing\n",
        "            3. Global context processing\n",
        "            4. Hierarchical-sequential fusion\n",
        "            5. Final sequence processing\n",
        "            \"\"\"\n",
        "            B, L = input_ids.shape\n",
        "\n",
        "            # ============= Level 1: Embeddings =============\n",
        "            positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            hidden_states = self.embed(input_ids) + self.pos_embed(positions)\n",
        "            hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "            # ============= Level 2: Chunk Processing =============\n",
        "            chunks = self._create_hierarchical_chunks(hidden_states, input_ids)\n",
        "            encoded_chunks = self.chunk_encoder(chunks)\n",
        "\n",
        "            # ============= Level 3: Global Context =============\n",
        "            global_context = self.global_encoder(encoded_chunks)\n",
        "\n",
        "            # ============= Level 4: Hierarchical-Sequential Bridge =============\n",
        "            # Project hierarchical features back to sequence space\n",
        "            projected_chunks = self.chunk_projection(global_context)\n",
        "\n",
        "            # Expand chunks back to sequence length\n",
        "            chunk_expanded = torch.repeat_interleave(projected_chunks, self.chunk_size, dim=1)\n",
        "            if chunk_expanded.size(1) > L:\n",
        "                chunk_expanded = chunk_expanded[:, :L, :]\n",
        "            elif chunk_expanded.size(1) < L:\n",
        "                padding = torch.zeros(B, L - chunk_expanded.size(1), self.embed_dim,\n",
        "                                    device=hidden_states.device)\n",
        "                chunk_expanded = torch.cat([chunk_expanded, padding], dim=1)\n",
        "\n",
        "            # Gated fusion between hierarchical and sequential features\n",
        "            gate_input = torch.cat([hidden_states, chunk_expanded], dim=-1)\n",
        "            gate = self.hierarchical_gate(gate_input)\n",
        "            combined = gate * chunk_expanded + (1 - gate) * hidden_states\n",
        "\n",
        "            # ============= Level 5: Final Sequence Processing =============\n",
        "            final_hidden = self.sequence_encoder(combined)\n",
        "\n",
        "            # Output projection\n",
        "            logits = self.output(final_hidden)\n",
        "\n",
        "            # Compute loss if labels provided\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"Robust generation method matching other models\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # Stopping conditions\n",
        "                    if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:\n",
        "                        if (next_token == self.tokenizer.eos_token_id).any():\n",
        "                            break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "    return PureHNetModel(vocab_size, tokenizer)"
      ],
      "metadata": {
        "id": "beCkNIoJ7_D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsection 4c: HNet-GPT2 Hybrid (Our Innovation)\n",
        "## HNet-GPT2 Hybrid: Our Novel Architecture\n",
        "\n",
        "**The main contribution**: Combines hierarchical encoding with proven GPT-2 generation:\n",
        "\n",
        "### Architecture Philosophy\n",
        "- **Hierarchical Encoder**: Code-aware adaptive chunking for structure understanding\n",
        "- **GPT-2 Decoder**: Proven autoregressive generation capabilities  \n",
        "- **Smart Fusion**: Complexity-aware gating and dynamic scaling\n",
        "\n",
        "### Key Innovations\n",
        "1. **Adaptive Chunking**: Code structure boundaries (def/class/if/for) vs fixed windows\n",
        "2. **Pre-trained Leverage**: Uses GPT-2 blocks instead of training from scratch\n",
        "3. **Gradual Training**: Strategic unfreezing for optimal learning\n",
        "4. **Complexity-Aware Integration**: Dynamic fusion based on code complexity\n",
        "\n",
        "### Why It Works\n",
        "- **Structure + Generation**: Gets hierarchical understanding AND proven generation\n",
        "- **Best of Both Worlds**: Combines strengths of both parent architectures\n",
        "- **Smart Training**: Leverages existing knowledge while adding structure awareness\n"
      ],
      "metadata": {
        "id": "urvxzbkq8XIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hnet_gpt2_hybrid(vocab_size, tokenizer):\n",
        "    \"\"\"HNet encoder with GPT-2 decoder blocks\"\"\"\n",
        "\n",
        "    class HNetGPT2Hybrid(nn.Module):\n",
        "        def __init__(self, vocab_size, tokenizer, embed_dim=768, num_chunks=24):\n",
        "            super().__init__()\n",
        "            self.tokenizer = tokenizer  # Store as instance variable\n",
        "\n",
        "            # Use GPT-2's embedding dimension for compatibility\n",
        "            self.embed_dim = embed_dim\n",
        "\n",
        "            # Embeddings (matching GPT-2's setup)\n",
        "            self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "            self.pos_embed = nn.Embedding(1024, embed_dim)\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "            # ============= HNet Encoder (from original) =============\n",
        "            self.num_chunks = 32  # Smaller chunks for better granularity\n",
        "            self.chunk_size = 512 // self.num_chunks\n",
        "            self.chunk_overlap = int(self.chunk_size * 0.25)  # 25% overlap\n",
        "\n",
        "\n",
        "            # Hierarchical chunk encoder (from HNet)\n",
        "            encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,  # Match GPT-2's attention heads\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.chunk_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "            # ============= GPT-2 Decoder Blocks =============\n",
        "            # Load GPT-2 configuration and extract decoder blocks\n",
        "            gpt2_config = GPT2Config(\n",
        "                vocab_size=vocab_size,\n",
        "                n_embd=embed_dim,\n",
        "                n_layer=12,  # Use 12 layers like GPT-2 small\n",
        "                n_head=12,\n",
        "                activation_function='gelu_new',\n",
        "                resid_pdrop=0.1,\n",
        "                embd_pdrop=0.1,\n",
        "                attn_pdrop=0.1,\n",
        "            )\n",
        "\n",
        "            # Create GPT-2 model and extract transformer blocks\n",
        "            gpt2_model = GPT2Model(gpt2_config)\n",
        "            self.gpt2_blocks = gpt2_model.h  # Extract the transformer blocks\n",
        "            self.ln_f = gpt2_model.ln_f  # Final layer norm\n",
        "\n",
        "\n",
        "            self.hierarchical_projection = nn.Sequential(\n",
        "                nn.Linear(embed_dim, embed_dim // 2),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(embed_dim // 2, embed_dim)\n",
        "            )\n",
        "\n",
        "            # Add dropout to gating:\n",
        "            self.hierarchical_gate = nn.Sequential(\n",
        "                nn.Linear(embed_dim * 2, embed_dim),\n",
        "                nn.Dropout(0.1),  # Add dropout here\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            # Initialize bias to favor original representations\n",
        "            nn.init.constant_(self.hierarchical_gate[0].bias, -2.0)\n",
        "\n",
        "\n",
        "            # Output projection\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "            # Initialize weights\n",
        "            self.apply(self._init_weights)\n",
        "\n",
        "        def _init_weights(self, module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "\n",
        "        def _create_adaptive_chunks(self, hidden_states, input_ids):\n",
        "            \"\"\"Create chunks based on code structure boundaries\"\"\"\n",
        "            B, L, D = hidden_states.shape\n",
        "            chunks = []\n",
        "\n",
        "            for batch_idx in range(B):\n",
        "                # Decode tokens back to text to find structure\n",
        "                try:\n",
        "                    # Get non-padded tokens\n",
        "                    tokens = input_ids[batch_idx]\n",
        "                    text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "                    # Find function/class boundaries using simple heuristics\n",
        "                    boundaries = [0]\n",
        "                    lines = text.split('\\n')\n",
        "                    current_pos = 0\n",
        "\n",
        "                    for line in lines:\n",
        "                        if line.strip().startswith(('def ', 'class ', 'if ', 'for ', 'while ')):\n",
        "                            # Convert line position back to token position (approximation)\n",
        "                            char_pos = text.find(line)\n",
        "                            if char_pos > 0:\n",
        "                                # Rough token position estimate\n",
        "                                token_pos = min(int(char_pos * 0.3), L-1)  # Rough char-to-token ratio\n",
        "                                if token_pos > boundaries[-1] + 10:  # Minimum chunk size\n",
        "                                    boundaries.append(token_pos)\n",
        "\n",
        "                    boundaries.append(L)\n",
        "\n",
        "                except:\n",
        "                    # Fallback to fixed chunking if parsing fails\n",
        "                    boundaries = [i * (L // self.num_chunks) for i in range(self.num_chunks + 1)]\n",
        "                    boundaries[-1] = L\n",
        "\n",
        "                # Create chunks from boundaries\n",
        "                batch_chunks = []\n",
        "                for i in range(len(boundaries) - 1):\n",
        "                    start, end = boundaries[i], boundaries[i + 1]\n",
        "\n",
        "                    # Add overlap for non-first chunks\n",
        "                    if i > 0:\n",
        "                        start = max(0, start - self.chunk_overlap)\n",
        "\n",
        "                    # Add overlap for non-last chunks\n",
        "                    if i < len(boundaries) - 2:\n",
        "                        end = min(L, end + self.chunk_overlap)\n",
        "\n",
        "                    if end > start:\n",
        "                        chunk_tokens = hidden_states[batch_idx, start:end]\n",
        "                        chunk_repr = torch.mean(chunk_tokens, dim=0)\n",
        "                        batch_chunks.append(chunk_repr)\n",
        "\n",
        "                # Pad to consistent number of chunks\n",
        "                while len(batch_chunks) < self.num_chunks:\n",
        "                    batch_chunks.append(torch.zeros(D, device=hidden_states.device))\n",
        "\n",
        "                # Take first num_chunks if we have too many\n",
        "                batch_chunks = batch_chunks[:self.num_chunks]\n",
        "                chunks.append(torch.stack(batch_chunks))\n",
        "            #\n",
        "            return chunks  # chunks is already a list of tensors for each batch\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            B, L = input_ids.shape\n",
        "\n",
        "            # ============= Embeddings =============\n",
        "            positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            hidden_states = self.embed(input_ids) + self.pos_embed(positions)\n",
        "            hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "            # ============= HNet Hierarchical Encoding =============\n",
        "\n",
        "\n",
        "            # Create adaptive chunks based on code structure\n",
        "            chunks = self._create_adaptive_chunks(hidden_states, input_ids)\n",
        "            chunk_tensor = torch.stack(chunks, dim=0)  # dim=0 for batch dimension\n",
        "            encoded_chunks = self.chunk_encoder(chunk_tensor)\n",
        "\n",
        "\n",
        "            # ============= GPT-2 Decoder with Cross-Attention =============\n",
        "            # Before\n",
        "            # First, apply cross-attention to incorporate hierarchical info\n",
        "            # Now\n",
        "            # Expand chunk representations back to sequence length\n",
        "            chunk_expanded = torch.repeat_interleave(encoded_chunks, self.chunk_size, dim=1)\n",
        "            if chunk_expanded.size(1) > L:\n",
        "                chunk_expanded = chunk_expanded[:, :L, :]\n",
        "            elif chunk_expanded.size(1) < L:\n",
        "                padding = torch.zeros(B, L - chunk_expanded.size(1), self.embed_dim, device=hidden_states.device)\n",
        "                chunk_expanded = torch.cat([chunk_expanded, padding], dim=1)\n",
        "\n",
        "            # Project hierarchical features\n",
        "            hierarchical_features = self.hierarchical_projection(chunk_expanded)\n",
        "\n",
        "            # Learned gating\n",
        "            gate_input = torch.cat([hidden_states, hierarchical_features], dim=-1)\n",
        "            gate = self.hierarchical_gate(gate_input)\n",
        "\n",
        "            # With this (residual scaling):\n",
        "            gated_hierarchical = gate * hierarchical_features + (1 - gate) * hidden_states\n",
        "\n",
        "            complexity_score = torch.mean(gate, dim=-1, keepdim=True)  # [B, L, 1]\n",
        "            alpha = 0.8 + 0.2 * complexity_score  # Dynamic 0.8-1.0 range\n",
        "            hidden_states = alpha * hidden_states + (1 - alpha) * gated_hierarchical\n",
        "\n",
        "\n",
        "            # Apply GPT-2 transformer blocks\n",
        "            for block in self.gpt2_blocks:\n",
        "                outputs = block(hidden_states)\n",
        "                hidden_states = outputs[0]\n",
        "\n",
        "            # Final layer norm\n",
        "            hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "            # Output projection\n",
        "            logits = self.output(hidden_states)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "            ##\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"FIXED: Properly reference self.tokenizer\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # FIXED: Use self.tokenizer instead of tokenizer\n",
        "                    if self.tokenizer.eos_token_id is not None and (next_token == self.tokenizer.eos_token_id).any():\n",
        "                        break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return HNetGPT2Hybrid(vocab_size, tokenizer)"
      ],
      "metadata": {
        "id": "fCE_IQyE8e1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Section 5: Training Infrastructure\n",
        " # Training Infrastructure\n",
        "\n",
        "Advanced training setup with:\n",
        "- **Gradual Unfreezing**: Strategic component unfreezing for hybrid model\n",
        "- **Warmup Scheduling**: Learning rate warmup + cosine decay\n",
        "- **Gradient Clipping**: Stable training for large models\n",
        "- **Model Checkpointing**: Automatic saving to Google Drive"
      ],
      "metadata": {
        "id": "qvCYBt5783mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, device, epochs=25, lr=1e-4, model_name=\"Model\"):\n",
        "    \"\"\"Train model with warmup + freezing strategy\"\"\"\n",
        "\n",
        "    # Phase 1: Freeze GPT-2 blocks\n",
        "    if hasattr(model, 'gpt2_blocks'):\n",
        "        for block in model.gpt2_blocks:\n",
        "            for param in block.parameters():\n",
        "                param.requires_grad = False\n",
        "        print(\"Phase 1: GPT-2 blocks frozen, training hierarchical components only\")\n",
        "\n",
        "    # Calculate warmup steps\n",
        "    warmup_epochs = 2\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = len(train_loader) * warmup_epochs\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=lr * 0.01,  # Start with very small LR\n",
        "        betas=(0.9, 0.95), weight_decay=0.05\n",
        "    )\n",
        "\n",
        "    # Warmup + Cosine scheduler\n",
        "    from transformers import get_linear_schedule_with_warmup\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Unfreezing schedule (same as before)\n",
        "        if hasattr(model, 'gpt2_blocks') and epoch == 8:\n",
        "            print(\"Phase 2: Unfreezing last 6 GPT-2 blocks\")\n",
        "            for block in model.gpt2_blocks[-6:]:\n",
        "                for param in block.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        elif hasattr(model, 'gpt2_blocks') and epoch == 16:\n",
        "            print(\"Phase 3: Unfreezing all GPT-2 blocks\")\n",
        "            for block in model.gpt2_blocks:\n",
        "                for param in block.parameters():\n",
        "                    param.requires_grad = True\n",
        "            # Update optimizer\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                lr=lr * 0.3,\n",
        "                betas=(0.9, 0.95), weight_decay=0.05\n",
        "            )\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_batches = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for input_ids, labels in pbar:\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits, loss = model(input_ids, labels=labels)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Step scheduler every batch during warmup\n",
        "                if step < total_steps:\n",
        "                    scheduler.step()\n",
        "                    step += 1\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_batches += 1\n",
        "\n",
        "                # Show current LR in progress bar\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'lr': f'{current_lr:.6f}'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Training error: {e}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = epoch_loss / max(epoch_batches, 1)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"   Epoch {epoch+1}: Loss = {avg_loss:.4f}, LR = {current_lr:.6f}\")\n",
        "\n",
        "    # Save model\n",
        "    save_path = f\"/content/drive/MyDrive/hnet_gpt2_models/{model_name.replace(' ', '_')}.pt\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Saved model to {save_path}\")\n",
        "\n",
        "    return {'losses': losses, 'final_loss': losses[-1] if losses else float('inf')}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate model on test data\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            try:\n",
        "                logits, loss = model(input_ids, labels=labels)\n",
        "\n",
        "                # Count tokens\n",
        "                mask = (labels != -100).float()\n",
        "                total_loss += loss.item() * mask.sum().item()\n",
        "                total_tokens += mask.sum().item()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Evaluation error: {e}\")\n",
        "                continue\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 100))  # Cap to avoid overflow\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_results(results):\n",
        "    \"\"\"Print benchmark results\"\"\"\n",
        "\n",
        "    print(f\"\\nHYBRID ARCHITECTURE RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Sort by perplexity\n",
        "    sorted_results = sorted([r for r in results if r['test_perplexity'] != float('inf')],\n",
        "                           key=lambda x: x['test_perplexity'])\n",
        "\n",
        "    print(f\"FINAL RANKING:\")\n",
        "    print(f\"{'Rank':<5} {'Model':<30} {'Perplexity':<12} {'Params':<10} {'Time(s)':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for i, result in enumerate(sorted_results, 1):\n",
        "        print(f\"{i:<5} {result['name']:<30} {result['test_perplexity']:<12.2f} \"\n",
        "              f\"{result['parameters']:>9,} {result['training_time']:<10.1f}\")\n",
        "\n",
        "    # Compare hybrid vs pure GPT-2\n",
        "    hybrid_result = next((r for r in results if 'Hybrid' in r['name']), None)\n",
        "    gpt2_result = next((r for r in results if 'Pure' in r['name']), None)\n",
        "\n",
        "    if hybrid_result and gpt2_result:\n",
        "        improvement = ((gpt2_result['test_perplexity'] - hybrid_result['test_perplexity']) /\n",
        "                      gpt2_result['test_perplexity']) * 100\n",
        "\n",
        "        print(f\"\\nHYBRID ARCHITECTURE ANALYSIS:\")\n",
        "        print(f\"HNet-GPT2-Hybrid:  {hybrid_result['test_perplexity']:.2f} perplexity\")\n",
        "        print(f\"Pure GPT-2:        {gpt2_result['test_perplexity']:.2f} perplexity\")\n",
        "        print(f\"Improvement:       {improvement:.1f}%\")\n",
        "\n",
        "        print(f\"\\nARCHITECTURAL INSIGHTS:\")\n",
        "        print(f\"• HNet hierarchical encoding: Provides chunk-level context\")\n",
        "        print(f\"• GPT-2 decoder blocks: Leverages pretrained architecture\")\n",
        "        print(f\"• Cross-attention fusion: Combines hierarchical + sequential\")\n",
        "        print(f\"• Real MBPP data: Tests on actual code understanding tasks\")"
      ],
      "metadata": {
        "id": "TklX1fgL88uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 6: Three-Way Architecture Comparison\n",
        "# Three-Way Architecture Comparison\n",
        "\n",
        "**The main experiment**: Comprehensive evaluation of all three approaches:\n",
        "\n",
        "## Research Question\n",
        "Which approach works best for code generation?\n",
        "1. **Pure hierarchical processing** (Pure HNet)\n",
        "2. **Pure sequential processing** (Pure GPT-2)  \n",
        "3. **Hybrid hierarchical-sequential** (Our HNet-GPT2)\n",
        "\n",
        "## Evaluation Metrics\n",
        "- **Perplexity**: Language modeling performance\n",
        "- **BLEU Score**: Generation quality vs ground truth\n",
        "- **Syntax Validity**: Percentage of syntactically correct code\n",
        "- **Function Completion**: Quality of function body completion\n",
        "\n",
        "## Experimental Setup\n",
        "- **Dataset**: MBPP (CodeSearchNet Python)\n",
        "- **Training**: 25 epochs with adaptive learning rate\n",
        "- **Hardware**: GPU acceleration with CUDA\n",
        "- **Reproducibility**: Fixed random seeds and comprehensive logging"
      ],
      "metadata": {
        "id": "7JKfkY879bwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_enhanced_three_way_comparison():\n",
        "    \"\"\"\n",
        "    Enhanced comprehensive comparison with proper evaluation hierarchy\n",
        "    \"\"\"\n",
        "    print(\"\\nLAUNCHING ENHANCED THREE-WAY COMPARISON\")\n",
        "    print(\"=\" * 55)\n",
        "    print(\"PRIMARY METRIC: Functional Correctness\")\n",
        "    print(\"SECONDARY METRICS: Code Quality & Syntax\")\n",
        "    print(\"REFERENCE METRIC: BLEU (structural similarity only)\")\n",
        "    print()\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Load tokenizer and setup\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    print(f\"Vocabulary size: {vocab_size:,}\")\n",
        "\n",
        "    # Architecture configurations (same as before)\n",
        "    models_config = [\n",
        "        {\n",
        "            'name': 'Pure HNet',\n",
        "            'model': create_pure_hnet_model(vocab_size, tokenizer),\n",
        "            'lr': 1e-4,\n",
        "            'description': 'End-to-end hierarchical transformer (4-level processing)',\n",
        "            'color': '#FF9500',\n",
        "            'innovation': 'Multi-level hierarchy with attention pooling'\n",
        "        },\n",
        "        {\n",
        "            'name': 'HNet-GPT2-Hybrid',\n",
        "            'model': create_hnet_gpt2_hybrid(vocab_size, tokenizer),\n",
        "            'lr': 1e-4,\n",
        "            'description': 'Hierarchical encoder + GPT-2 decoder blocks',\n",
        "            'color': '#FF6B6B',\n",
        "            'innovation': 'Best of both worlds: hierarchy + proven GPT-2'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Pure GPT-2',\n",
        "            'model': create_pure_gpt2_baseline(vocab_size),\n",
        "            'lr': 1e-4,\n",
        "            'description': 'Standard sequential transformer (baseline)',\n",
        "            'color': '#45B7D1',\n",
        "            'innovation': 'Proven architecture for autoregressive generation'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"\\nLoading dataset for code generation evaluation...\")\n",
        "    train_loader, test_loader = create_mbpp_data_loaders(tokenizer, device)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for i, config in enumerate(models_config, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"MODEL {i}/3: {config['name']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\" {config['description']}\")\n",
        "        print(f\" Innovation: {config['innovation']}\")\n",
        "\n",
        "        model = config['model'].to(device)\n",
        "        params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"Parameters: {params:,}\")\n",
        "\n",
        "        # Training (same logic as before)\n",
        "        model_path = f\"/content/drive/MyDrive/hnet_gpt2_models/{config['name'].replace(' ', '_')}.pt\"\n",
        "        model_exists = os.path.exists(model_path)\n",
        "\n",
        "        if model_exists:\n",
        "            print(f\"Loading existing model from {model_path}\")\n",
        "            try:\n",
        "                model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "                training_time = 0\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load model: {e}, training new model...\")\n",
        "                model_exists = False\n",
        "\n",
        "        if not model_exists:\n",
        "            print(f\"\\nTRAINING {config['name']}...\")\n",
        "            start_time = time.time()\n",
        "            training_history = train_model(\n",
        "                model, train_loader, device,\n",
        "                epochs=25, lr=config['lr'], model_name=config['name']\n",
        "            )\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "        # ENHANCED EVALUATION\n",
        "        print(f\"\\n🔬 ENHANCED EVALUATION: {config['name']}\")\n",
        "        comprehensive_results = evaluate_model_comprehensive_enhanced(\n",
        "            model, test_loader, device, tokenizer, config['name']\n",
        "        )\n",
        "\n",
        "        # Store results with proper emphasis\n",
        "        result = {\n",
        "            'name': config['name'],\n",
        "            'description': config['description'],\n",
        "            'innovation': config['innovation'],\n",
        "            'training_time': training_time,\n",
        "            'parameters': params,\n",
        "            'color': config['color'],\n",
        "\n",
        "            # PRIMARY METRICS (for ranking)\n",
        "            'functional_accuracy': comprehensive_results['functional_accuracy'],\n",
        "            'perplexity': comprehensive_results['perplexity'],\n",
        "\n",
        "            # SECONDARY METRICS (code quality)\n",
        "            'syntax_validity': comprehensive_results['syntax_validity'],\n",
        "            'complexity_score': comprehensive_results['complexity_score'],\n",
        "            'pep8_compliance': comprehensive_results['pep8_compliance'],\n",
        "            'readability': comprehensive_results['readability'],\n",
        "\n",
        "            # REFERENCE METRICS (structural similarity)\n",
        "            'bleu_score': comprehensive_results['bleu_score'],\n",
        "            'ast_similarity': comprehensive_results['ast_similarity'],\n",
        "            'identifier_similarity': comprehensive_results['identifier_similarity'],\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    # Enhanced analysis\n",
        "    print_enhanced_analysis(results)\n",
        "    return results\n",
        "\n",
        "def print_enhanced_analysis(results):\n",
        "    \"\"\"Enhanced analysis with proper metric hierarchy\"\"\"\n",
        "\n",
        "    print(f\"\\nENHANCED THREE-WAY ARCHITECTURE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # PRIMARY RANKING: Functional Correctness\n",
        "    print(f\"\\nPRIMARY RANKING (Functional Correctness):\")\n",
        "    print(f\"{'Rank':<5} {'Architecture':<20} {'Functional':<12} {'Perplexity':<12} {'Syntax':<8}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Sort by functional correctness first, then perplexity\n",
        "    sorted_by_function = sorted(results, key=lambda x: (-x['functional_accuracy'], x['perplexity']))\n",
        "\n",
        "    for i, result in enumerate(sorted_by_function, 1):\n",
        "        emoji = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\"\n",
        "        print(f\"{emoji} {i:<3} {result['name']:<20} {result['functional_accuracy']:<12.1%} \"\n",
        "              f\"{result['perplexity']:<12.2f} {result['syntax_validity']:<8.1%}\")\n",
        "\n",
        "    # SECONDARY RANKING: Overall Code Quality\n",
        "    print(f\"\\nSECONDARY RANKING (Code Quality):\")\n",
        "    print(f\"{'Rank':<5} {'Architecture':<20} {'Quality':<10} {'PEP8':<8} {'Readability':<12}\")\n",
        "    print(\"-\" * 58)\n",
        "\n",
        "    # Calculate composite quality score\n",
        "    for result in results:\n",
        "        quality_score = (result['syntax_validity'] * 0.4 +\n",
        "                        result['pep8_compliance'] * 0.3 +\n",
        "                        result['readability'] * 0.3)\n",
        "        result['quality_composite'] = quality_score\n",
        "\n",
        "    sorted_by_quality = sorted(results, key=lambda x: -x['quality_composite'])\n",
        "\n",
        "    for i, result in enumerate(sorted_by_quality, 1):\n",
        "        emoji = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\"\n",
        "        print(f\"{emoji} {i:<3} {result['name']:<20} {result['quality_composite']:<10.1%} \"\n",
        "              f\"{result['pep8_compliance']:<8.1%} {result['readability']:<12.1%}\")\n",
        "\n",
        "    # REFERENCE RANKING: Structural Similarity\n",
        "    print(f\"\\n📝 REFERENCE METRICS (Structural Similarity):\")\n",
        "    print(f\"{'Architecture':<20} {'BLEU':<8} {'AST':<8} {'Identifiers':<12}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for result in results:\n",
        "        print(f\"{result['name']:<20} {result['bleu_score']:<8.3f} \"\n",
        "              f\"{result['ast_similarity']:<8.3f} {result['identifier_similarity']:<12.3f}\")\n",
        "\n",
        "    print(f\"\\n⚠️ Important: BLEU scores measure structural similarity to reference code,\")\n",
        "    print(f\"   NOT functional correctness. Lower BLEU + high functional accuracy\")\n",
        "    print(f\"   indicates the model found different but valid solutions.\")\n",
        "\n",
        "    # WINNER ANALYSIS\n",
        "    winner = sorted_by_function[0]\n",
        "    print(f\"\\n🏆 OVERALL WINNER: {winner['name']}\")\n",
        "    print(f\"🎯 Functional Accuracy: {winner['functional_accuracy']:.1%}\")\n",
        "    print(f\"📈 Perplexity: {winner['perplexity']:.2f}\")\n",
        "    print(f\"✅ Code Quality: {winner['quality_composite']:.1%}\")\n",
        "\n",
        "    # Compare functional accuracy improvements\n",
        "    print(f\"\\n📈 FUNCTIONAL CORRECTNESS ANALYSIS:\")\n",
        "    best_functional = max(results, key=lambda x: x['functional_accuracy'])\n",
        "\n",
        "    for result in results:\n",
        "        if result != best_functional:\n",
        "            improvement = best_functional['functional_accuracy'] - result['functional_accuracy']\n",
        "            print(f\"   {best_functional['name']} vs {result['name']}: +{improvement:.1%} functional accuracy\")"
      ],
      "metadata": {
        "id": "9_POKeL69tv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 7: Quick Execution & Results\n",
        "# Quick Execution & Results\n",
        "\n",
        "Run this section to reproduce our main results:\n",
        "\n",
        "## One-Click Execution"
      ],
      "metadata": {
        "id": "geX4h4Ml-hc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_enhanced_quick_execution():\n",
        "    \"\"\"\n",
        "    Quick execution with enhanced evaluation metrics\n",
        "    \"\"\"\n",
        "    print(\"🚀 ENHANCED HNET-GPT EVALUATION\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"✅ Enhanced with functional correctness evaluation\")\n",
        "    print(\"📊 Proper metric hierarchy: Function > Quality > Structure\")\n",
        "    print(\"⚠️  BLEU now correctly contextualized as structural similarity\")\n",
        "    print(\"⏱️  Expected runtime: ~3-4 hours on Tesla T4\")\n",
        "    print()\n",
        "\n",
        "    # Run the enhanced comparison\n",
        "    results = run_enhanced_three_way_comparison()\n",
        "\n",
        "    # Enhanced final summary\n",
        "    print(\"\\n🎉 ENHANCED EXPERIMENT COMPLETE!\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    winner = max(results, key=lambda x: x['functional_accuracy'])\n",
        "\n",
        "    print(f\"🏆 WINNER: {winner['name']}\")\n",
        "    print(f\"   🎯 Functional Accuracy: {winner['functional_accuracy']:.1%}\")\n",
        "    print(f\"   📈 Perplexity: {winner['perplexity']:.2f}\")\n",
        "    print(f\"   ✅ Syntax Validity: {winner['syntax_validity']:.1%}\")\n",
        "    print(f\"   📝 BLEU (structural): {winner['bleu_score']:.3f}\")\n",
        "\n",
        "    print(f\"\\n📋 KEY IMPROVEMENTS IN THIS EVALUATION:\")\n",
        "    print(f\"   ✅ Functional correctness as primary metric\")\n",
        "    print(f\"   📊 Multi-tier evaluation hierarchy\")\n",
        "    print(f\"   ⚠️  BLEU properly contextualized\")\n",
        "    print(f\"   🧪 Actual test case execution\")\n",
        "    print(f\"   📈 Code quality assessment\")\n",
        "\n",
        "    return results\n",
        "\n",
        "enhanced_results = run_enhanced_three_way_comparison()\n"
      ],
      "metadata": {
        "id": "OJIzvUjVhXSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enhanced_results = show_enhanced_results()"
      ],
      "metadata": {
        "id": "7-kb3pEBqXZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}