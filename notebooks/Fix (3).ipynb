{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HNet-GPT: Structure-Aware Code Generation via Hierarchical Encoding and Transformer Decoding\n",
        "Models available at: [your_github_link]/models/\n",
        "\n"
      ],
      "metadata": {
        "id": "UB-jX5BAn_3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0: Environment Setup & Dependencies\n",
        "**First, install required packages:**"
      ],
      "metadata": {
        "id": "kyJZLyap_TJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install datasets==2.16.1\n",
        "!pip install huggingface_hub --upgrade\n",
        "!pip install nltk\n",
        "!pip install transformers>=4.21.0\n",
        "!pip install radon\n",
        "\n",
        "# Verify installations\n",
        "import datasets\n",
        "import transformers\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")"
      ],
      "metadata": {
        "id": "71SsYGiI_jUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Introduction and Setup #\n",
        "\n",
        "**Exploratory Research** - Novel hybrid architecture combining hierarchical encoding with GPT-2 generation\n",
        "\n",
        "This notebook stats the discussion on the HNet-GPT architecture.\n",
        "\n",
        "## Research Overview\n",
        "This work introduces **HNet-GPT**, a novel hybrid architecture that combines:\n",
        "- **Hierarchical Encoding (HNet)**: Structure-aware code understanding\n",
        "- **Sequential Generation (GPT-2)**: Proven autoregressive text generation\n",
        "- **Adaptive Fusion**: Smart integration of both approaches\n",
        "\n",
        "## Key Results Preview\n",
        "- **40.6%** better than Pure GPT-2 (8.20 vs 13.80 perplexity)\n",
        "- **39.5%** better than Pure HNet (8.20 vs 13.56 perplexity)\n",
        "\n",
        "## Architecture Comparison\n",
        "Here are evaluated three fundamental approaches:\n",
        "1. **Pure HNet**: End-to-end hierarchical transformer\n",
        "2. **HNet-GPT2 Hybrid**: The novel approach\n",
        "3. **Pure GPT-2**: Sequential transformer baseline\n"
      ],
      "metadata": {
        "id": "hOVo5BEVsjqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.makedirs(\"/content/drive/MyDrive/hnet_gpt2_models\", exist_ok=True)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Model, GPT2Config, AutoTokenizer\n",
        "import time\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import ast\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import ast\n",
        "import sys\n",
        "from io import StringIO\n",
        "import contextlib\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import radon.complexity as radon_complexity\n",
        "from radon.visitors import ComplexityVisitor"
      ],
      "metadata": {
        "id": "8DedGFc0suGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Section 2: Core Utilities & Helper Functions\n",
        "\n",
        "This section contains essential utility functions used across all models:\n",
        "- **Top-k/Top-p filtering**: Advanced text generation sampling\n",
        "- **Data loading utilities**: MBPP dataset integration\n",
        "* **Code-focused evaluation**: Code token identification, pattern recognition, and syntax validation\n"
      ],
      "metadata": {
        "id": "Y5k70HaFuZUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k/Top-p filtering function\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf'), min_tokens_to_keep=1):\n",
        "    \"\"\"\n",
        "    Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "    Args:\n",
        "        logits: logits distribution shape (batch_size, vocabulary size)\n",
        "        top_k: (int or float) If set to int > 0, only the top_k least likely tokens are kept per batch item.\n",
        "               If set to float (0.0 < top_k < 1.0), the number of tokens kept per batch item represents\n",
        "               the percentage of the vocabulary size (e.g. 0.2 means keep 20% of the vocabulary).\n",
        "        top_p: (float) If set to < 1.0, only the most likely tokens with probabilities that add up to >= top_p are\n",
        "               kept for generation.\n",
        "        filter_value: (float) value that will be used to fill filtered tokens.\n",
        "        min_tokens_to_keep: (int) Minimum number of tokens that cannot be filtered.\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        if not isinstance(top_k, int):\n",
        "            top_k = int(top_k * logits.shape[-1]) # Convert to absolute number of tokens\n",
        "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1)) # Clamp to (min_tokens_to_keep, vocab_size)\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        if min_tokens_to_keep > 1:\n",
        "            # Keep at least min_tokens_to_keep (set to false the first min_tokens_to_keep)\n",
        "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
        "        # Shift the indices to the right to keep the first token above the threshold\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation functions\n",
        "def check_syntax_validity(code_text):\n",
        "    \"\"\"Check if generated code is syntactically valid Python\"\"\"\n",
        "    try:\n",
        "        ast.parse(code_text)\n",
        "        return True\n",
        "    except SyntaxError:\n",
        "        return False\n",
        "\n",
        "\n",
        "# 1. CODE TOKEN IDENTIFICATION FUNCTION\n",
        "# Add this after your existing utility functions in Section 2\n",
        "\n",
        "def get_comprehensive_code_tokens(tokenizer):\n",
        "    \"\"\"Identify all code-relevant tokens in the vocabulary\"\"\"\n",
        "\n",
        "    print(\"Identifying code-relevant tokens...\")\n",
        "\n",
        "    # All possible code elements\n",
        "    code_elements = [\n",
        "        # Single characters\n",
        "        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
        "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "\n",
        "        # Operators and symbols\n",
        "        '+', '-', '*', '/', '%', '=', '==', '!=', '>', '<', '>=', '<=',\n",
        "        '(', ')', '[', ']', '{', '}', '.', ',', ':', ';', '_', '\"', \"'\",\n",
        "\n",
        "        # Whitespace\n",
        "        ' ', '\\n', '\\t',\n",
        "\n",
        "        # Keywords\n",
        "        'return', 'if', 'else', 'elif', 'for', 'while', 'def', 'class',\n",
        "        'True', 'False', 'None', 'and', 'or', 'not', 'in', 'is',\n",
        "\n",
        "        # Built-in functions\n",
        "        'len', 'str', 'int', 'float', 'bool', 'list', 'dict', 'set',\n",
        "        'range', 'sum', 'max', 'min', 'abs', 'round',\n",
        "        'print', 'input', 'open',\n",
        "\n",
        "        # Common method names\n",
        "        'upper', 'lower', 'strip', 'split', 'join', 'replace', 'append', 'remove',\n",
        "\n",
        "        # With spaces (GPT-2 tokenizer includes spaces)\n",
        "        ' +', ' -', ' *', ' /', ' %', ' =', ' ==', ' !=', ' >', ' <', ' >=', ' <=',\n",
        "        ' a', ' b', ' c', ' x', ' y', ' z', ' n', ' i', ' j', ' k',\n",
        "        ' return', ' if', ' else', ' True', ' False', ' None',\n",
        "        ' 0', ' 1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9',\n",
        "    ]\n",
        "\n",
        "    # Get token IDs\n",
        "    code_token_ids = set()\n",
        "    for element in code_elements:\n",
        "        try:\n",
        "            token_ids = tokenizer.encode(element, add_special_tokens=False)\n",
        "            code_token_ids.update(token_ids)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Add common combinations that work\n",
        "    common_combinations = [\n",
        "        'a +', 'b +', 'x *', 'n %', '== 0', '> 0', '< 0',\n",
        "        'return a', 'return x', 'return n', 'return True', 'return False',\n",
        "        'len(', 'str(', 'int(', 'abs(', 'x)', 'a)', 'b)', 'n)',\n",
        "    ]\n",
        "\n",
        "    for combo in common_combinations:\n",
        "        try:\n",
        "            token_ids = tokenizer.encode(combo, add_special_tokens=False)\n",
        "            code_token_ids.update(token_ids)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    code_token_list = sorted(list(code_token_ids))\n",
        "    print(f\"Identified {len(code_token_list)} code-relevant tokens\")\n",
        "\n",
        "    # Test coverage for common code\n",
        "    test_code = \"a + b\"\n",
        "    test_tokens = tokenizer.encode(test_code, add_special_tokens=False)\n",
        "    overlap = len(set(test_tokens) & set(code_token_list))\n",
        "    print(f\"   Test coverage 'a + b': {overlap}/{len(test_tokens)} tokens ({overlap/len(test_tokens):.1%})\")\n",
        "\n",
        "    return code_token_list\n",
        "\n",
        "# ============================================================================\n",
        "# 2. CODE-FOCUSED EVALUATION FUNCTION\n",
        "# Add this function to evaluate your existing models\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_with_code_focus(model, tokenizer, device, code_token_ids, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate existing models with code-focused approach\"\"\"\n",
        "\n",
        "    print(f\"\\nCODE-FOCUSED EVALUATION: {model_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test prompts for code completion\n",
        "    test_cases = [\n",
        "        {\n",
        "            'prompt': 'def add(a, b):\\n    return ',\n",
        "            'expected_patterns': ['a', 'b', '+', 'a + b'],\n",
        "            'description': 'Basic addition'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def is_even(n):\\n    return ',\n",
        "            'expected_patterns': ['n', '%', '2', '==', '0', 'n % 2 == 0'],\n",
        "            'description': 'Boolean logic'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def max_two(x, y):\\n    if x > y:\\n        return ',\n",
        "            'expected_patterns': ['x', 'y', 'if', 'else'],\n",
        "            'description': 'Conditional logic'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def square(n):\\n    return ',\n",
        "            'expected_patterns': ['n', '*', 'n * n'],\n",
        "            'description': 'Simple expression'\n",
        "        },\n",
        "        {\n",
        "            'prompt': 'def is_positive(x):\\n    return ',\n",
        "            'expected_patterns': ['x', '>', '0', 'x > 0'],\n",
        "            'description': 'Comparison'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Results tracking\n",
        "    total_tests = len(test_cases)\n",
        "    syntax_successes = 0\n",
        "    pattern_successes = 0\n",
        "    code_token_successes = 0\n",
        "\n",
        "    code_token_ids_set = set(code_token_ids)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, test_case in enumerate(test_cases, 1):\n",
        "            print(f\"\\nTest {i}: {test_case['description']}\")\n",
        "            print(f\"   Prompt: {test_case['prompt'].strip()}\")\n",
        "\n",
        "            prompt_tokens = tokenizer(test_case['prompt'], return_tensors='pt').to(device)\n",
        "\n",
        "            # Try different generation strategies\n",
        "            best_completion = \"\"\n",
        "            best_score = 0\n",
        "\n",
        "            strategies = [\n",
        "                {\"temperature\": 0.1, \"do_sample\": False, \"name\": \"Greedy\"},\n",
        "                {\"temperature\": 0.3, \"do_sample\": True, \"top_k\": 20, \"name\": \"Conservative\"},\n",
        "                {\"temperature\": 0.7, \"do_sample\": True, \"top_k\": 50, \"name\": \"Creative\"}\n",
        "            ]\n",
        "\n",
        "            for strategy in strategies:\n",
        "                try:\n",
        "                    # Generate with existing model method\n",
        "                    if hasattr(model, 'generate'):\n",
        "                        generated = model.generate(\n",
        "                            prompt_tokens.input_ids,\n",
        "                            max_length=prompt_tokens.input_ids.size(1) + 15,\n",
        "                            repetition_penalty=1.1,\n",
        "                            eos_token_id=tokenizer.eos_token_id,\n",
        "                            pad_token_id=tokenizer.eos_token_id,\n",
        "                            **{k: v for k, v in strategy.items() if k != 'name'}\n",
        "                        )\n",
        "                    else:\n",
        "                        # Fallback for models without generate method\n",
        "                        continue\n",
        "\n",
        "                    full_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "                    completion = full_text[len(test_case['prompt']):].strip()\n",
        "\n",
        "                    if completion and len(completion) > len(best_completion):\n",
        "                        best_completion = completion\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            print(f\"   Generated: '{best_completion}'\")\n",
        "\n",
        "            if not best_completion:\n",
        "                print(f\"No generation produced\")\n",
        "                continue\n",
        "\n",
        "            # Test 1: Syntax validity\n",
        "            try:\n",
        "                full_code = test_case['prompt'] + best_completion\n",
        "                ast.parse(full_code)\n",
        "                print(f\"Valid syntax\")\n",
        "                syntax_successes += 1\n",
        "            except SyntaxError:\n",
        "                print(f\"Syntax error\")\n",
        "\n",
        "            # Test 2: Contains expected patterns\n",
        "            pattern_found = False\n",
        "            for pattern in test_case['expected_patterns']:\n",
        "                if pattern in best_completion:\n",
        "                    print(f\"Contains expected pattern: '{pattern}'\")\n",
        "                    pattern_found = True\n",
        "                    break\n",
        "\n",
        "            if pattern_found:\n",
        "                pattern_successes += 1\n",
        "            else:\n",
        "                print(f\"No expected patterns found\")\n",
        "\n",
        "            # Test 3: Uses code tokens\n",
        "            completion_tokens = tokenizer.encode(best_completion, add_special_tokens=False)\n",
        "            code_token_overlap = len(set(completion_tokens) & code_token_ids_set)\n",
        "            code_token_ratio = code_token_overlap / len(completion_tokens) if completion_tokens else 0\n",
        "\n",
        "            print(f\"Code token usage: {code_token_overlap}/{len(completion_tokens)} ({code_token_ratio:.1%})\")\n",
        "\n",
        "            if code_token_ratio >= 0.5:  # At least 50% code tokens\n",
        "                print(f\"Good code token usage\")\n",
        "                code_token_successes += 1\n",
        "            else:\n",
        "                print(f\"Low code token usage\")\n",
        "\n",
        "    # Calculate final scores\n",
        "    syntax_rate = syntax_successes / total_tests\n",
        "    pattern_rate = pattern_successes / total_tests\n",
        "    code_token_rate = code_token_successes / total_tests\n",
        "\n",
        "    # Composite score (weighted)\n",
        "    composite_score = (syntax_rate * 0.4 + pattern_rate * 0.4 + code_token_rate * 0.2)\n",
        "\n",
        "    print(f\"\\nCODE-FOCUSED RESULTS FOR {model_name}:\")\n",
        "    print(f\"Syntax Validity: {syntax_successes}/{total_tests} ({syntax_rate:.1%})\")\n",
        "    print(f\"Pattern Recognition: {pattern_successes}/{total_tests} ({pattern_rate:.1%})\")\n",
        "    print(f\"Code Token Usage: {code_token_successes}/{total_tests} ({code_token_rate:.1%})\")\n",
        "    print(f\"Composite Score: {composite_score:.1%}\")\n",
        "\n",
        "    return {\n",
        "        'syntax_rate': syntax_rate,\n",
        "        'pattern_rate': pattern_rate,\n",
        "        'code_token_rate': code_token_rate,\n",
        "        'composite_score': composite_score\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ENHANCED RESULTS COMPARISON\n",
        "# Add this to compare all your existing models\n",
        "# ============================================================================\n",
        "\n",
        "def show_enhanced_results():\n",
        "    \"\"\"Show enhanced evaluation of your existing trained models\"\"\"\n",
        "\n",
        "    print(\"ENHANCED EVALUATION OF EXISTING MODELS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Testing your trained models with code-focused evaluation\")\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Get code tokens\n",
        "    code_token_ids = get_comprehensive_code_tokens(tokenizer)\n",
        "\n",
        "    # Test your existing models\n",
        "    models_to_test = [\n",
        "        {\n",
        "            'name': 'Pure HNet',\n",
        "            'creator': lambda: create_pure_hnet_model(vocab_size, tokenizer),\n",
        "            'file': 'Pure_HNet.pt'\n",
        "        },\n",
        "        {\n",
        "            'name': 'HNet-GPT2-Hybrid',\n",
        "            'creator': lambda: create_hnet_gpt2_hybrid(vocab_size, tokenizer),\n",
        "            'file': 'HNet-GPT2-Hybrid.pt'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Pure GPT-2',\n",
        "            'creator': lambda: create_pure_gpt2_baseline(vocab_size),\n",
        "            'file': 'Pure_GPT-2.pt'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_config in models_to_test:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TESTING: {model_config['name']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Create model\n",
        "            model = model_config['creator']().to(device)\n",
        "\n",
        "            # Load existing weights\n",
        "            model_path = f\"/content/drive/MyDrive/hnet_gpt2_models/{model_config['file']}\"\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            print(f\"Loaded existing model weights\")\n",
        "\n",
        "            # Enhanced evaluation\n",
        "            enhanced_results = evaluate_with_code_focus(\n",
        "                model, tokenizer, device, code_token_ids, model_config['name']\n",
        "            )\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'name': model_config['name'],\n",
        "                'loaded': True,\n",
        "                **enhanced_results\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Clear GPU memory\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to test {model_config['name']}: {e}\")\n",
        "            results.append({\n",
        "                'name': model_config['name'],\n",
        "                'loaded': False,\n",
        "                'composite_score': 0.0\n",
        "            })\n",
        "\n",
        "    # Show final comparison\n",
        "    print(f\"\\nENHANCED RESULTS COMPARISON\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Model':<20} {'Syntax':<8} {'Patterns':<10} {'Tokens':<8} {'Overall':<8}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    # Sort by composite score\n",
        "    sorted_results = sorted([r for r in results if r['loaded']],\n",
        "                           key=lambda x: x['composite_score'], reverse=True)\n",
        "\n",
        "    for i, result in enumerate(sorted_results, 1):\n",
        "        emoji = \"ðŸ¥‡\" if i == 1 else \"ðŸ¥ˆ\" if i == 2 else \"ðŸ¥‰\"\n",
        "        print(f\"{emoji} {result['name']:<18} {result['syntax_rate']:<8.1%} \"\n",
        "              f\"{result['pattern_rate']:<10.1%} {result['code_token_rate']:<8.1%} \"\n",
        "              f\"{result['composite_score']:<8.1%}\")\n",
        "\n",
        "    if sorted_results:\n",
        "        winner = sorted_results[0]\n",
        "        print(f\"\\nBEST PERFORMING MODEL: {winner['name']}\")\n",
        "        print(f\"   Overall Score: {winner['composite_score']:.1%}\")\n",
        "\n",
        "        if winner['composite_score'] > 0.5:\n",
        "            print(f\"STRONG PERFORMANCE! Model shows good code generation capability\")\n",
        "        elif winner['composite_score'] > 0.25:\n",
        "            print(f\"MODERATE PERFORMANCE! Model shows some code awareness\")\n",
        "        else:\n",
        "            print(f\"LEARNING OPPORTUNITY! Model needs code-focused training\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "pt9uXj9quilU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Section 3: Data Loading & Preprocessing #\n",
        " This section handles the MBPP (Mostly Basic Python Programs) dataset:\n",
        "- **Primary**: CodeSearchNet Python dataset (4,000 train + 800 test examples)\n",
        "- **Fallback**: High-quality synthetic Python code patterns\n",
        "- **Format**: Task description + code solution pairs\n",
        "- **Preprocessing**: Tokenization with GPT-2 tokenizer"
      ],
      "metadata": {
        "id": "fZrZa1uUytoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to import datasets, but don't fail if it's not available\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    DATASETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DATASETS_AVAILABLE = False\n",
        "    print(\"Datasets library not available, will use synthetic data\")\n",
        "\n",
        "\n",
        "\n",
        "def create_mbpp_data_loaders(tokenizer, device, max_length=512, batch_size=4):\n",
        "    \"\"\"Create data loaders for MBPP dataset or synthetic code data\"\"\"\n",
        "\n",
        "    class CodeDataset(Dataset):\n",
        "        def __init__(self, split='train', max_length=512):\n",
        "            self.examples = []\n",
        "            data_loaded = False\n",
        "\n",
        "            # Approach 1: Try loading CodeSearchNet Python (now with correct field names!)\n",
        "            if DATASETS_AVAILABLE and not data_loaded:\n",
        "                try:\n",
        "                    print(f\"Attempting to load CodeSearchNet Python {split} split...\")\n",
        "                    dataset = load_dataset('code_search_net', 'python', split=split, trust_remote_code=True)\n",
        "\n",
        "                    count = 0\n",
        "                    max_examples = 4000 if split == 'train' else 800  # Double the data\n",
        "\n",
        "                    for item in tqdm(dataset, desc=f\"Processing CodeSearchNet {split} data\"):\n",
        "                        if count >= max_examples:\n",
        "                            break\n",
        "\n",
        "                        # Use the CORRECT field names from the debug output\n",
        "                        code = item.get('func_code_string', '').strip()\n",
        "                        func_name = item.get('func_name', '').strip()\n",
        "                        docstring = item.get('func_documentation_string', '').strip()\n",
        "\n",
        "                        # Basic filtering for valid Python functions\n",
        "                        if code and len(code) > 30 and 'def ' in code:\n",
        "                            # Create a task-like format\n",
        "                            if docstring and len(docstring) > 5:\n",
        "                                text = f\"# Task: {docstring}\\n\\n# Solution:\\n{code}\"\n",
        "                            else:\n",
        "                                text = f\"# Function: {func_name}\\n\\n# Solution:\\n{code}\"\n",
        "\n",
        "                            # Tokenize\n",
        "                            tokens = tokenizer(\n",
        "                                text,\n",
        "                                truncation=True,\n",
        "                                max_length=max_length,\n",
        "                                padding='max_length',\n",
        "                                return_tensors='pt'\n",
        "                            )\n",
        "\n",
        "                            self.examples.append({\n",
        "                                'input_ids': tokens['input_ids'].squeeze(0),\n",
        "                                'labels': tokens['input_ids'].squeeze(0).clone()\n",
        "                            })\n",
        "                            count += 1\n",
        "\n",
        "                    if len(self.examples) > 0:\n",
        "                        data_loaded = True\n",
        "                        print(f\"Successfully loaded {len(self.examples)} CodeSearchNet examples for {split}\")\n",
        "                    else:\n",
        "                        print(f\"No valid examples found in CodeSearchNet {split}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load CodeSearchNet: {e}\")\n",
        "\n",
        "            # Approach 2: Create high-quality synthetic Python code data\n",
        "\n",
        "\n",
        "            ###\n",
        "            if not data_loaded:\n",
        "                print(f\"Creating synthetic Python code data for {split}...\")\n",
        "                num_examples = 500 if split == 'train' else 100\n",
        "\n",
        "                # More diverse and realistic code patterns\n",
        "                code_patterns = [\n",
        "                    # Function definitions\n",
        "                    \"def calculate_sum(numbers):\\n    '''Calculate sum of a list'''\\n    total = 0\\n    for num in numbers:\\n        total += num\\n    return total\",\n",
        "\n",
        "                    # Classes\n",
        "                    \"class DataProcessor:\\n    def __init__(self, data):\\n        self.data = data\\n        self.processed = False\\n    \\n    def process(self):\\n        self.data = [x * 2 for x in self.data]\\n        self.processed = True\",\n",
        "\n",
        "                    # List comprehensions and algorithms\n",
        "                    \"def find_primes(n):\\n    '''Find all prime numbers up to n'''\\n    primes = []\\n    for num in range(2, n + 1):\\n        is_prime = True\\n        for i in range(2, int(num ** 0.5) + 1):\\n            if num % i == 0:\\n                is_prime = False\\n                break\\n        if is_prime:\\n            primes.append(num)\\n    return primes\",\n",
        "\n",
        "                    # Recursion\n",
        "                    \"def fibonacci(n):\\n    '''Calculate nth Fibonacci number'''\\n    if n <= 1:\\n        return n\\n    return fibonacci(n - 1) + fibonacci(n - 2)\",\n",
        "\n",
        "                    # String manipulation\n",
        "                    \"def reverse_words(sentence):\\n    '''Reverse words in a sentence'''\\n    words = sentence.split()\\n    reversed_words = words[::-1]\\n    return ' '.join(reversed_words)\",\n",
        "\n",
        "                    # Dictionary operations\n",
        "                    \"def count_frequencies(items):\\n    '''Count frequency of each item'''\\n    freq_dict = {}\\n    for item in items:\\n        if item in freq_dict:\\n            freq_dict[item] += 1\\n        else:\\n            freq_dict[item] = 1\\n    return freq_dict\",\n",
        "\n",
        "                    # Error handling\n",
        "                    \"def safe_divide(a, b):\\n    '''Safely divide two numbers'''\\n    try:\\n        result = a / b\\n        return result\\n    except ZeroDivisionError:\\n        return None\\n    except TypeError:\\n        return 'Invalid input types'\",\n",
        "\n",
        "                    # Sorting algorithms\n",
        "                    \"def bubble_sort(arr):\\n    '''Implement bubble sort'''\\n    n = len(arr)\\n    for i in range(n):\\n        for j in range(0, n - i - 1):\\n            if arr[j] > arr[j + 1]:\\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\\n    return arr\",\n",
        "                ]\n",
        "\n",
        "                for i in range(num_examples):\n",
        "                    # Rotate through patterns and add variations\n",
        "                    base_code = code_patterns[i % len(code_patterns)]\n",
        "\n",
        "                    # Add task description\n",
        "                    task_descriptions = [\n",
        "                        \"Write a function to solve this problem\",\n",
        "                        \"Implement a solution for the following\",\n",
        "                        \"Create a Python function that handles this task\",\n",
        "                        \"Develop an algorithm to compute the result\",\n",
        "                    ]\n",
        "\n",
        "                    task = task_descriptions[i % len(task_descriptions)]\n",
        "                    full_text = f\"# Task: {task}\\n\\n{base_code}\"\n",
        "\n",
        "                    # Tokenize\n",
        "                    tokens = tokenizer(\n",
        "                        full_text,\n",
        "                        truncation=True,\n",
        "                        max_length=max_length,\n",
        "                        padding='max_length',\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "\n",
        "                    self.examples.append({\n",
        "                        'input_ids': tokens['input_ids'].squeeze(0),\n",
        "                        'labels': tokens['input_ids'].squeeze(0).clone()\n",
        "                    })\n",
        "\n",
        "                print(f\"Created {len(self.examples)} synthetic examples for {split}\")\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.examples)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.examples[idx]['input_ids'], self.examples[idx]['labels']\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\nPreparing datasets...\")\n",
        "    train_dataset = CodeDataset('train', max_length)\n",
        "    test_dataset = CodeDataset('test', max_length)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Avoid multiprocessing issues\n",
        "        pin_memory=True if device == 'cuda' else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True if device == 'cuda' else False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"   Training examples: {len(train_dataset)}\")\n",
        "    print(f\"   Test examples: {len(test_dataset)}\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   Max sequence length: {max_length}\")\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "3aM7aUrd6zft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Model Architectures"
      ],
      "metadata": {
        "id": "CTw1_1R27ZuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pure GPT-2 Baseline Model\n",
        "\n",
        "Standard sequential transformer architecture:\n",
        "- **12-layer GPT-2** configuration\n",
        "- **Sequential processing** of code tokens\n",
        "- **No hierarchical structure**\n",
        "- **Baseline performance** for comparison"
      ],
      "metadata": {
        "id": "g2cSrXlU7eTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pure_gpt2_baseline(vocab_size):\n",
        "    \"\"\"Pure GPT-2 baseline for comparison\"\"\"\n",
        "\n",
        "    class PureGPT2(nn.Module):\n",
        "        def __init__(self, vocab_size, embed_dim=768):\n",
        "            super().__init__()\n",
        "\n",
        "            # Create GPT-2 configuration\n",
        "            self.config = GPT2Config(\n",
        "                vocab_size=vocab_size,\n",
        "                n_embd=embed_dim,\n",
        "                n_layer=12,\n",
        "                n_head=12,\n",
        "                activation_function='gelu_new',\n",
        "                resid_pdrop=0.1,\n",
        "                embd_pdrop=0.1,\n",
        "                attn_pdrop=0.1,\n",
        "            )\n",
        "\n",
        "            # Create GPT-2 model\n",
        "            self.gpt2 = GPT2Model(self.config)\n",
        "\n",
        "            # Output projection\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            # Forward through GPT-2\n",
        "            outputs = self.gpt2(input_ids)\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "\n",
        "            # Project to vocabulary\n",
        "            logits = self.output(hidden_states)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "            ##\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"FIXED: Use parameter instead of tokenizer reference\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "            eos_token_id = kwargs.get('eos_token_id', 50256)  # FIXED: Get EOS token from kwargs\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # FIXED: Use eos_token_id parameter instead of tokenizer\n",
        "                    if eos_token_id is not None and (next_token == eos_token_id).any():\n",
        "                        break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "\n",
        "    return PureGPT2(vocab_size)"
      ],
      "metadata": {
        "id": "BZQE3JXf7k2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pure HNet: End-to-End Hierarchical Architecture\n",
        "\n",
        "Novel hierarchical transformer with 4-level processing:\n",
        "1. **Chunk-Level Processing**: Fixed overlapping windows with attention pooling\n",
        "2. **Global Context**: Inter-chunk relationship modeling  \n",
        "3. **Hierarchical-Sequential Bridge**: Project back to sequence space\n",
        "4. **Final Sequence Processing**: Custom transformer layers\n",
        "\n",
        "**Key Innovations:**\n",
        "- Attention-based chunk pooling (vs mean pooling)\n",
        "- Explicit global context stage\n",
        "- End-to-end hierarchical optimization"
      ],
      "metadata": {
        "id": "BbiHLkkv74gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pure_hnet_model(vocab_size, tokenizer):\n",
        "    \"\"\"\n",
        "    Create Pure HNet model - End-to-end hierarchical transformer\n",
        "\n",
        "    Architecture Philosophy:\n",
        "    - Multi-level hierarchical processing (chunk â†’ global â†’ sequence)\n",
        "    - Attention-based chunk pooling for better representations\n",
        "    - Gated fusion between hierarchical and sequential features\n",
        "    - Designed specifically for structured text like code\n",
        "    \"\"\"\n",
        "\n",
        "    class PureHNetModel(nn.Module):\n",
        "        def __init__(self, vocab_size, tokenizer, embed_dim=768):\n",
        "            super().__init__()\n",
        "            self.tokenizer = tokenizer\n",
        "            self.embed_dim = embed_dim\n",
        "\n",
        "            print(f\"Building Pure HNet Architecture:\")\n",
        "            print(f\"Vocabulary: {vocab_size:,} tokens\")\n",
        "            print(f\"Embedding dimension: {embed_dim}\")\n",
        "\n",
        "            # ============= Core Embeddings =============\n",
        "            self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "            self.pos_embed = nn.Embedding(1024, embed_dim)\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "            # ============= Hierarchical Parameters =============\n",
        "            self.num_chunks = 32  # Number of hierarchical chunks\n",
        "            self.chunk_size = 512 // self.num_chunks\n",
        "            self.chunk_overlap = int(self.chunk_size * 0.25)  # 25% overlap\n",
        "\n",
        "            print(f\"Chunking strategy: {self.num_chunks} chunks of size {self.chunk_size}\")\n",
        "            print(f\"Chunk overlap: {self.chunk_overlap} tokens\")\n",
        "\n",
        "            # ============= Level 1: Chunk-Level Processing =============\n",
        "            chunk_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.chunk_encoder = nn.TransformerEncoder(chunk_encoder_layer, num_layers=6)\n",
        "\n",
        "            # ============= Level 2: Global Context Processing =============\n",
        "            global_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.global_encoder = nn.TransformerEncoder(global_encoder_layer, num_layers=4)\n",
        "\n",
        "            # ============= Level 3: Hierarchical-to-Sequential Bridge =============\n",
        "            self.chunk_projection = nn.Sequential(\n",
        "                nn.Linear(embed_dim, embed_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(embed_dim, embed_dim)\n",
        "            )\n",
        "\n",
        "            # Gating mechanism for hierarchical fusion\n",
        "            self.hierarchical_gate = nn.Sequential(\n",
        "                nn.Linear(embed_dim * 2, embed_dim),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            # ============= Level 4: Final Sequence Processing =============\n",
        "            sequence_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.sequence_encoder = nn.TransformerEncoder(sequence_encoder_layer, num_layers=6)\n",
        "\n",
        "            # ============= Output Layer =============\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "            # Initialize weights\n",
        "            self.apply(self._init_weights)\n",
        "\n",
        "            total_params = sum(p.numel() for p in self.parameters())\n",
        "            print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "        def _init_weights(self, module):\n",
        "            \"\"\"Initialize model weights with small random values\"\"\"\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "        def _create_hierarchical_chunks(self, hidden_states, input_ids):\n",
        "            \"\"\"\n",
        "            Create hierarchical chunks with smart overlapping\n",
        "\n",
        "            Key Innovation: Attention-based pooling instead of mean pooling\n",
        "            for better chunk representations\n",
        "            \"\"\"\n",
        "            B, L, D = hidden_states.shape\n",
        "            chunks = []\n",
        "\n",
        "            for batch_idx in range(B):\n",
        "                batch_chunks = []\n",
        "\n",
        "                # Create overlapping chunks for better context\n",
        "                for i in range(self.num_chunks):\n",
        "                    start = i * self.chunk_size\n",
        "                    end = min(start + self.chunk_size + self.chunk_overlap, L)\n",
        "\n",
        "                    if start < L:\n",
        "                        chunk_tokens = hidden_states[batch_idx, start:end]\n",
        "                        # Use attention pooling for better representations\n",
        "                        chunk_repr = self._attention_pool(chunk_tokens)\n",
        "                        batch_chunks.append(chunk_repr)\n",
        "\n",
        "                # Ensure consistent chunk count\n",
        "                while len(batch_chunks) < self.num_chunks:\n",
        "                    batch_chunks.append(torch.zeros(D, device=hidden_states.device))\n",
        "\n",
        "                batch_chunks = batch_chunks[:self.num_chunks]\n",
        "                chunks.append(torch.stack(batch_chunks))\n",
        "\n",
        "            return torch.stack(chunks, dim=0)\n",
        "\n",
        "        def _attention_pool(self, chunk_tokens):\n",
        "            \"\"\"\n",
        "            Attention-based pooling for chunk representation\n",
        "\n",
        "            Better than mean pooling as it focuses on important tokens\n",
        "            \"\"\"\n",
        "            if chunk_tokens.size(0) == 0:\n",
        "                return torch.zeros(self.embed_dim, device=chunk_tokens.device)\n",
        "\n",
        "            # Compute attention weights based on token importance\n",
        "            chunk_mean = chunk_tokens.mean(dim=0, keepdim=True)\n",
        "            attention_scores = torch.sum(chunk_tokens * chunk_mean, dim=-1)\n",
        "            attention_weights = torch.softmax(attention_scores, dim=0)\n",
        "\n",
        "            # Weighted sum using attention\n",
        "            return torch.sum(chunk_tokens * attention_weights.unsqueeze(-1), dim=0)\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            \"\"\"\n",
        "            Four-level hierarchical forward pass:\n",
        "            1. Embeddings + Chunking\n",
        "            2. Chunk-level processing\n",
        "            3. Global context processing\n",
        "            4. Hierarchical-sequential fusion\n",
        "            5. Final sequence processing\n",
        "            \"\"\"\n",
        "            B, L = input_ids.shape\n",
        "\n",
        "            # ============= Level 1: Embeddings =============\n",
        "            positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            hidden_states = self.embed(input_ids) + self.pos_embed(positions)\n",
        "            hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "            # ============= Level 2: Chunk Processing =============\n",
        "            chunks = self._create_hierarchical_chunks(hidden_states, input_ids)\n",
        "            encoded_chunks = self.chunk_encoder(chunks)\n",
        "\n",
        "            # ============= Level 3: Global Context =============\n",
        "            global_context = self.global_encoder(encoded_chunks)\n",
        "\n",
        "            # ============= Level 4: Hierarchical-Sequential Bridge =============\n",
        "            # Project hierarchical features back to sequence space\n",
        "            projected_chunks = self.chunk_projection(global_context)\n",
        "\n",
        "            # Expand chunks back to sequence length\n",
        "            chunk_expanded = torch.repeat_interleave(projected_chunks, self.chunk_size, dim=1)\n",
        "            if chunk_expanded.size(1) > L:\n",
        "                chunk_expanded = chunk_expanded[:, :L, :]\n",
        "            elif chunk_expanded.size(1) < L:\n",
        "                padding = torch.zeros(B, L - chunk_expanded.size(1), self.embed_dim,\n",
        "                                    device=hidden_states.device)\n",
        "                chunk_expanded = torch.cat([chunk_expanded, padding], dim=1)\n",
        "\n",
        "            # Gated fusion between hierarchical and sequential features\n",
        "            gate_input = torch.cat([hidden_states, chunk_expanded], dim=-1)\n",
        "            gate = self.hierarchical_gate(gate_input)\n",
        "            combined = gate * chunk_expanded + (1 - gate) * hidden_states\n",
        "\n",
        "            # ============= Level 5: Final Sequence Processing =============\n",
        "            final_hidden = self.sequence_encoder(combined)\n",
        "\n",
        "            # Output projection\n",
        "            logits = self.output(final_hidden)\n",
        "\n",
        "            # Compute loss if labels provided\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"Robust generation method matching other models\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # Stopping conditions\n",
        "                    if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:\n",
        "                        if (next_token == self.tokenizer.eos_token_id).any():\n",
        "                            break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "    return PureHNetModel(vocab_size, tokenizer)"
      ],
      "metadata": {
        "id": "beCkNIoJ7_D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##HNet-GPT2 Hybrid: the Novel Architecture\n",
        "\n",
        "**The main contribution**: Combines hierarchical encoding with proven GPT-2 generation:\n",
        "\n",
        "### Architecture Philosophy\n",
        "- **Hierarchical Encoder**: Code-aware adaptive chunking for structure understanding\n",
        "- **GPT-2 Decoder**: Proven autoregressive generation capabilities  \n",
        "- **Smart Fusion**: Complexity-aware gating and dynamic scaling\n",
        "\n",
        "### Key Innovations\n",
        "1. **Adaptive Chunking**: Code structure boundaries (def/class/if/for) vs fixed windows\n",
        "2. **Pre-trained Leverage**: Uses GPT-2 blocks instead of training from scratch\n",
        "3. **Gradual Training**: Strategic unfreezing for optimal learning\n",
        "4. **Complexity-Aware Integration**: Dynamic fusion based on code complexity\n",
        "\n",
        "### Why It Works\n",
        "- **Structure + Generation**: Gets hierarchical understanding AND proven generation\n",
        "- **Best of Both Worlds**: Combines strengths of both parent architectures\n",
        "- **Smart Training**: Leverages existing knowledge while adding structure awareness\n"
      ],
      "metadata": {
        "id": "urvxzbkq8XIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hnet_gpt2_hybrid(vocab_size, tokenizer):\n",
        "    \"\"\"HNet encoder with GPT-2 decoder blocks\"\"\"\n",
        "\n",
        "    class HNetGPT2Hybrid(nn.Module):\n",
        "        def __init__(self, vocab_size, tokenizer, embed_dim=768, num_chunks=24):\n",
        "            super().__init__()\n",
        "            self.tokenizer = tokenizer  # Store as instance variable\n",
        "\n",
        "            # Use GPT-2's embedding dimension for compatibility\n",
        "            self.embed_dim = embed_dim\n",
        "\n",
        "            # Embeddings (matching GPT-2's setup)\n",
        "            self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "            self.pos_embed = nn.Embedding(1024, embed_dim)\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "            # ============= HNet Encoder (from original) =============\n",
        "            self.num_chunks = 32  # Smaller chunks for better granularity\n",
        "            self.chunk_size = 512 // self.num_chunks\n",
        "            self.chunk_overlap = int(self.chunk_size * 0.25)  # 25% overlap\n",
        "\n",
        "\n",
        "            # Hierarchical chunk encoder (from HNet)\n",
        "            encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,  # Match GPT-2's attention heads\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.chunk_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "            # ============= GPT-2 Decoder Blocks =============\n",
        "            # Load GPT-2 configuration and extract decoder blocks\n",
        "            gpt2_config = GPT2Config(\n",
        "                vocab_size=vocab_size,\n",
        "                n_embd=embed_dim,\n",
        "                n_layer=12,  # Use 12 layers like GPT-2 small\n",
        "                n_head=12,\n",
        "                activation_function='gelu_new',\n",
        "                resid_pdrop=0.1,\n",
        "                embd_pdrop=0.1,\n",
        "                attn_pdrop=0.1,\n",
        "            )\n",
        "\n",
        "            # Create GPT-2 model and extract transformer blocks\n",
        "            gpt2_model = GPT2Model(gpt2_config)\n",
        "            self.gpt2_blocks = gpt2_model.h  # Extract the transformer blocks\n",
        "            self.ln_f = gpt2_model.ln_f  # Final layer norm\n",
        "\n",
        "\n",
        "            self.hierarchical_projection = nn.Sequential(\n",
        "                nn.Linear(embed_dim, embed_dim // 2),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(embed_dim // 2, embed_dim)\n",
        "            )\n",
        "\n",
        "            # Add dropout to gating:\n",
        "            self.hierarchical_gate = nn.Sequential(\n",
        "                nn.Linear(embed_dim * 2, embed_dim),\n",
        "                nn.Dropout(0.1),  # Add dropout here\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            # Initialize bias to favor original representations\n",
        "            nn.init.constant_(self.hierarchical_gate[0].bias, -2.0)\n",
        "\n",
        "\n",
        "            # Output projection\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "            # Initialize weights\n",
        "            self.apply(self._init_weights)\n",
        "\n",
        "        def _init_weights(self, module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "\n",
        "        def _create_adaptive_chunks(self, hidden_states, input_ids):\n",
        "            \"\"\"Create chunks based on code structure boundaries\"\"\"\n",
        "            B, L, D = hidden_states.shape\n",
        "            chunks = []\n",
        "\n",
        "            for batch_idx in range(B):\n",
        "                # Decode tokens back to text to find structure\n",
        "                try:\n",
        "                    # Get non-padded tokens\n",
        "                    tokens = input_ids[batch_idx]\n",
        "                    text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "                    # Find function/class boundaries using simple heuristics\n",
        "                    boundaries = [0]\n",
        "                    lines = text.split('\\n')\n",
        "                    current_pos = 0\n",
        "\n",
        "                    for line in lines:\n",
        "                        if line.strip().startswith(('def ', 'class ', 'if ', 'for ', 'while ')):\n",
        "                            # Convert line position back to token position (approximation)\n",
        "                            char_pos = text.find(line)\n",
        "                            if char_pos > 0:\n",
        "                                # Rough token position estimate\n",
        "                                token_pos = min(int(char_pos * 0.3), L-1)  # Rough char-to-token ratio\n",
        "                                if token_pos > boundaries[-1] + 10:  # Minimum chunk size\n",
        "                                    boundaries.append(token_pos)\n",
        "\n",
        "                    boundaries.append(L)\n",
        "\n",
        "                except:\n",
        "                    # Fallback to fixed chunking if parsing fails\n",
        "                    boundaries = [i * (L // self.num_chunks) for i in range(self.num_chunks + 1)]\n",
        "                    boundaries[-1] = L\n",
        "\n",
        "                # Create chunks from boundaries\n",
        "                batch_chunks = []\n",
        "                for i in range(len(boundaries) - 1):\n",
        "                    start, end = boundaries[i], boundaries[i + 1]\n",
        "\n",
        "                    # Add overlap for non-first chunks\n",
        "                    if i > 0:\n",
        "                        start = max(0, start - self.chunk_overlap)\n",
        "\n",
        "                    # Add overlap for non-last chunks\n",
        "                    if i < len(boundaries) - 2:\n",
        "                        end = min(L, end + self.chunk_overlap)\n",
        "\n",
        "                    if end > start:\n",
        "                        chunk_tokens = hidden_states[batch_idx, start:end]\n",
        "                        chunk_repr = torch.mean(chunk_tokens, dim=0)\n",
        "                        batch_chunks.append(chunk_repr)\n",
        "\n",
        "                # Pad to consistent number of chunks\n",
        "                while len(batch_chunks) < self.num_chunks:\n",
        "                    batch_chunks.append(torch.zeros(D, device=hidden_states.device))\n",
        "\n",
        "                # Take first num_chunks if we have too many\n",
        "                batch_chunks = batch_chunks[:self.num_chunks]\n",
        "                chunks.append(torch.stack(batch_chunks))\n",
        "            #\n",
        "            return chunks  # chunks is already a list of tensors for each batch\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            B, L = input_ids.shape\n",
        "\n",
        "            # ============= Embeddings =============\n",
        "            positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            hidden_states = self.embed(input_ids) + self.pos_embed(positions)\n",
        "            hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "            # ============= HNet Hierarchical Encoding =============\n",
        "\n",
        "\n",
        "            # Create adaptive chunks based on code structure\n",
        "            chunks = self._create_adaptive_chunks(hidden_states, input_ids)\n",
        "            chunk_tensor = torch.stack(chunks, dim=0)  # dim=0 for batch dimension\n",
        "            encoded_chunks = self.chunk_encoder(chunk_tensor)\n",
        "\n",
        "\n",
        "            # ============= GPT-2 Decoder with Cross-Attention =============\n",
        "            # Before\n",
        "            # First, apply cross-attention to incorporate hierarchical info\n",
        "            # Now\n",
        "            # Expand chunk representations back to sequence length\n",
        "            chunk_expanded = torch.repeat_interleave(encoded_chunks, self.chunk_size, dim=1)\n",
        "            if chunk_expanded.size(1) > L:\n",
        "                chunk_expanded = chunk_expanded[:, :L, :]\n",
        "            elif chunk_expanded.size(1) < L:\n",
        "                padding = torch.zeros(B, L - chunk_expanded.size(1), self.embed_dim, device=hidden_states.device)\n",
        "                chunk_expanded = torch.cat([chunk_expanded, padding], dim=1)\n",
        "\n",
        "            # Project hierarchical features\n",
        "            hierarchical_features = self.hierarchical_projection(chunk_expanded)\n",
        "\n",
        "            # Learned gating\n",
        "            gate_input = torch.cat([hidden_states, hierarchical_features], dim=-1)\n",
        "            gate = self.hierarchical_gate(gate_input)\n",
        "\n",
        "            # With this (residual scaling):\n",
        "            gated_hierarchical = gate * hierarchical_features + (1 - gate) * hidden_states\n",
        "\n",
        "            complexity_score = torch.mean(gate, dim=-1, keepdim=True)  # [B, L, 1]\n",
        "            alpha = 0.8 + 0.2 * complexity_score  # Dynamic 0.8-1.0 range\n",
        "            hidden_states = alpha * hidden_states + (1 - alpha) * gated_hierarchical\n",
        "\n",
        "\n",
        "            # Apply GPT-2 transformer blocks\n",
        "            for block in self.gpt2_blocks:\n",
        "                outputs = block(hidden_states)\n",
        "                hidden_states = outputs[0]\n",
        "\n",
        "            # Final layer norm\n",
        "            hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "            # Output projection\n",
        "            logits = self.output(hidden_states)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "            ##\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"FIXED: Properly reference self.tokenizer\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # FIXED: Use self.tokenizer instead of tokenizer\n",
        "                    if self.tokenizer.eos_token_id is not None and (next_token == self.tokenizer.eos_token_id).any():\n",
        "                        break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return HNetGPT2Hybrid(vocab_size, tokenizer)"
      ],
      "metadata": {
        "id": "fCE_IQyE8e1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Section 5: Training Infrastructure\n",
        "\n",
        "\n",
        "Advanced training setup with:\n",
        "- **Gradual Unfreezing**: Strategic component unfreezing for hybrid model\n",
        "- **Warmup Scheduling**: Learning rate warmup + cosine decay\n",
        "- **Gradient Clipping**: Stable training for large models\n",
        "- **Model Checkpointing**: Automatic saving to Google Drive"
      ],
      "metadata": {
        "id": "qvCYBt5783mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, device, epochs=25, lr=1e-4, model_name=\"Model\"):\n",
        "    \"\"\"Train model with warmup + freezing strategy\"\"\"\n",
        "\n",
        "    # Phase 1: Freeze GPT-2 blocks\n",
        "    if hasattr(model, 'gpt2_blocks'):\n",
        "        for block in model.gpt2_blocks:\n",
        "            for param in block.parameters():\n",
        "                param.requires_grad = False\n",
        "        print(\"Phase 1: GPT-2 blocks frozen, training hierarchical components only\")\n",
        "\n",
        "    # Calculate warmup steps\n",
        "    warmup_epochs = 2\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = len(train_loader) * warmup_epochs\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=lr * 0.01,  # Start with very small LR\n",
        "        betas=(0.9, 0.95), weight_decay=0.05\n",
        "    )\n",
        "\n",
        "    # Warmup + Cosine scheduler\n",
        "    from transformers import get_linear_schedule_with_warmup\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Unfreezing schedule (same as before)\n",
        "        if hasattr(model, 'gpt2_blocks') and epoch == 8:\n",
        "            print(\"Phase 2: Unfreezing last 6 GPT-2 blocks\")\n",
        "            for block in model.gpt2_blocks[-6:]:\n",
        "                for param in block.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        elif hasattr(model, 'gpt2_blocks') and epoch == 16:\n",
        "            print(\"Phase 3: Unfreezing all GPT-2 blocks\")\n",
        "            for block in model.gpt2_blocks:\n",
        "                for param in block.parameters():\n",
        "                    param.requires_grad = True\n",
        "            # Update optimizer\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                lr=lr * 0.3,\n",
        "                betas=(0.9, 0.95), weight_decay=0.05\n",
        "            )\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_batches = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for input_ids, labels in pbar:\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits, loss = model(input_ids, labels=labels)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Step scheduler every batch during warmup\n",
        "                if step < total_steps:\n",
        "                    scheduler.step()\n",
        "                    step += 1\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_batches += 1\n",
        "\n",
        "                # Show current LR in progress bar\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'lr': f'{current_lr:.6f}'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Training error: {e}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = epoch_loss / max(epoch_batches, 1)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"   Epoch {epoch+1}: Loss = {avg_loss:.4f}, LR = {current_lr:.6f}\")\n",
        "\n",
        "    # Save model\n",
        "    save_path = f\"/content/drive/MyDrive/hnet_gpt2_models/{model_name.replace(' ', '_')}.pt\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Saved model to {save_path}\")\n",
        "\n",
        "    return {'losses': losses, 'final_loss': losses[-1] if losses else float('inf')}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate model on test data\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            try:\n",
        "                logits, loss = model(input_ids, labels=labels)\n",
        "\n",
        "                # Count tokens\n",
        "                mask = (labels != -100).float()\n",
        "                total_loss += loss.item() * mask.sum().item()\n",
        "                total_tokens += mask.sum().item()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Evaluation error: {e}\")\n",
        "                continue\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 100))  # Cap to avoid overflow\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_results(results):\n",
        "    \"\"\"Print benchmark results\"\"\"\n",
        "\n",
        "    print(f\"\\nHYBRID ARCHITECTURE RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Sort by perplexity\n",
        "    sorted_results = sorted([r for r in results if r['test_perplexity'] != float('inf')],\n",
        "                           key=lambda x: x['test_perplexity'])\n",
        "\n",
        "    print(f\"FINAL RANKING:\")\n",
        "    print(f\"{'Rank':<5} {'Model':<30} {'Perplexity':<12} {'Params':<10} {'Time(s)':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for i, result in enumerate(sorted_results, 1):\n",
        "        print(f\"{i:<5} {result['name']:<30} {result['test_perplexity']:<12.2f} \"\n",
        "              f\"{result['parameters']:>9,} {result['training_time']:<10.1f}\")\n",
        "\n",
        "    # Compare hybrid vs pure GPT-2\n",
        "    hybrid_result = next((r for r in results if 'Hybrid' in r['name']), None)\n",
        "    gpt2_result = next((r for r in results if 'Pure' in r['name']), None)\n",
        "\n",
        "    if hybrid_result and gpt2_result:\n",
        "        improvement = ((gpt2_result['test_perplexity'] - hybrid_result['test_perplexity']) /\n",
        "                      gpt2_result['test_perplexity']) * 100\n",
        "\n",
        "        print(f\"\\nHYBRID ARCHITECTURE ANALYSIS:\")\n",
        "        print(f\"HNet-GPT2-Hybrid:  {hybrid_result['test_perplexity']:.2f} perplexity\")\n",
        "        print(f\"Pure GPT-2:        {gpt2_result['test_perplexity']:.2f} perplexity\")\n",
        "        print(f\"Improvement:       {improvement:.1f}%\")\n",
        "\n",
        "        print(f\"\\nARCHITECTURAL INSIGHTS:\")\n",
        "        print(f\"â€¢ HNet hierarchical encoding: Provides chunk-level context\")\n",
        "        print(f\"â€¢ GPT-2 decoder blocks: Leverages pretrained architecture\")\n",
        "        print(f\"â€¢ Cross-attention fusion: Combines hierarchical + sequential\")\n",
        "        print(f\"â€¢ Real MBPP data: Tests on actual code understanding tasks\")"
      ],
      "metadata": {
        "id": "TklX1fgL88uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Enhanced Code-Focused Evaluation\n",
        "\n",
        "**The main experiment**: Comprehensive evaluation of all three approaches:\n",
        "\n",
        "## Research Question\n",
        "Which approach works best for code generation?\n",
        "1. **Pure hierarchical processing** (Pure HNet)\n",
        "2. **Pure sequential processing** (Pure GPT-2)  \n",
        "3. **Hybrid hierarchical-sequential** (Our HNet-GPT2)\n",
        "\n",
        "\n",
        "## Evaluation Metrics\n",
        "* **Syntax Validity**: Whether generated code parses correctly\n",
        "* **Pattern Recognition**: Ability to complete common code patterns (e.g., `a + b`, `n % 2 == 0`)\n",
        "* **Code Token Usage**: Percentage of actual code tokens vs. natural language\n",
        "* **Composite Score**: Weighted combination of all metrics\n",
        "\n",
        "## Test Cases\n",
        "* Basic arithmetic operations (`def add(a, b): return ___`)\n",
        "* Boolean logic (`def is_even(n): return ___`)\n",
        "* Conditional statements (`def max_two(x, y): if x > y: return ___`)\n",
        "* Simple expressions (`def square(n): return ___`)\n",
        "* Comparison operations (`def is_positive(x): return ___`)\n",
        "\n",
        "\n",
        "## Experimental Setup\n",
        "- **Training**: 25 epochs with adaptive learning rate\n",
        "- **Generation Strategies**: Greedy, Conservative (temp=0.3), Creative (temp=0.7)\n",
        "- **Hardware**: GPU acceleration with CUDA\n",
        "- **Focus**: Practical code completion rather than abstract metrics\n"
      ],
      "metadata": {
        "id": "7JKfkY879bwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the enhanced evaluation on pre-trained models\n",
        "\n",
        "enhanced_results = show_enhanced_results()"
      ],
      "metadata": {
        "id": "9_POKeL69tv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}