{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0: Environment Setup & Dependencies #\n",
        "# Environment Setup & Dependencies\n",
        "\n",
        "**First, install required packages:**"
      ],
      "metadata": {
        "id": "kyJZLyap_TJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install datasets==2.16.1\n",
        "!pip install huggingface_hub --upgrade\n",
        "!pip install nltk\n",
        "!pip install transformers>=4.21.0\n",
        "\n",
        "# Verify installations\n",
        "import datasets\n",
        "import transformers\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")"
      ],
      "metadata": {
        "id": "71SsYGiI_jUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Introduction & Setup #\n",
        "# HNet-GPT: Structure-Aware Code Generation via Hierarchical Encoding and Transformer Decoding\n",
        "\n",
        "> **Preliminary Research** - Novel hybrid architecture combining hierarchical encoding with GPT-2 generation\n",
        ">\n",
        "> This notebook stats the discussion on the HNet-GPT architecture.\n",
        "\n",
        "## Research Overview\n",
        "This work introduces **HNet-GPT**, a novel hybrid architecture that combines:\n",
        "- **Hierarchical Encoding (HNet)**: Structure-aware code understanding\n",
        "- **Sequential Generation (GPT-2)**: Proven autoregressive text generation\n",
        "- **Adaptive Fusion**: Smart integration of both approaches\n",
        "\n",
        "## Key Results Preview\n",
        "- **40.6%** better than Pure GPT-2 (8.20 vs 13.80 perplexity)\n",
        "- **39.5%** better than Pure HNet (8.20 vs 13.56 perplexity)\n",
        "- Best performance across most code generation metrics\n",
        "\n",
        "## Architecture Comparison\n",
        "Here are evaluated three fundamental approaches:\n",
        "1. **Pure HNet**: End-to-end hierarchical transformer\n",
        "2. **HNet-GPT2 Hybrid**: The novel approach\n",
        "3. **Pure GPT-2**: Sequential transformer baseline\n",
        "\n",
        "## Current Limitations\n",
        "bla bla need more money time and better algorithms bla bla bla\n",
        "\n",
        "bro BLEU score are soo bad\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hOVo5BEVsjqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QPZywcSG_Qsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.makedirs(\"/content/drive/MyDrive/hnet_gpt2_models\", exist_ok=True)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Model, GPT2Config, AutoTokenizer\n",
        "import time\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import ast\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "8DedGFc0suGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Section 2: Core Utilities & Helper Functions #\n",
        " # Core Utilities & Helper Functions\n",
        "\n",
        "This section contains essential utility functions used across all models:\n",
        "- **Top-k/Top-p filtering**: Advanced text generation sampling\n",
        "- **Data loading utilities**: MBPP dataset integration\n",
        "- **Evaluation metrics**: BLEU, syntax validity, function completion"
      ],
      "metadata": {
        "id": "Y5k70HaFuZUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k/Top-p filtering function\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf'), min_tokens_to_keep=1):\n",
        "    \"\"\"\n",
        "    Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "    Args:\n",
        "        logits: logits distribution shape (batch_size, vocabulary size)\n",
        "        top_k: (int or float) If set to int > 0, only the top_k least likely tokens are kept per batch item.\n",
        "               If set to float (0.0 < top_k < 1.0), the number of tokens kept per batch item represents\n",
        "               the percentage of the vocabulary size (e.g. 0.2 means keep 20% of the vocabulary).\n",
        "        top_p: (float) If set to < 1.0, only the most likely tokens with probabilities that add up to >= top_p are\n",
        "               kept for generation.\n",
        "        filter_value: (float) value that will be used to fill filtered tokens.\n",
        "        min_tokens_to_keep: (int) Minimum number of tokens that cannot be filtered.\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        if not isinstance(top_k, int):\n",
        "            top_k = int(top_k * logits.shape[-1]) # Convert to absolute number of tokens\n",
        "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1)) # Clamp to (min_tokens_to_keep, vocab_size)\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        if min_tokens_to_keep > 1:\n",
        "            # Keep at least min_tokens_to_keep (set to false the first min_tokens_to_keep)\n",
        "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
        "        # Shift the indices to the right to keep the first token above the threshold\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation functions\n",
        "def check_syntax_validity(code_text):\n",
        "    \"\"\"Check if generated code is syntactically valid Python\"\"\"\n",
        "    try:\n",
        "        ast.parse(code_text)\n",
        "        return True\n",
        "    except SyntaxError:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_code_generation(model, test_loader, tokenizer, device, num_samples=10):\n",
        "    \"\"\"Fixed version that passes eos_token_id to Pure GPT-2\"\"\"\n",
        "    model.eval()\n",
        "    bleu_scores = []\n",
        "    exact_matches = 0\n",
        "    samples_processed = 0\n",
        "\n",
        "    print(\"ðŸ” DEBUGGING GENERATION QUALITY...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input_ids, labels) in enumerate(test_loader):\n",
        "            if samples_processed >= num_samples:\n",
        "                break\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "            batch_size = input_ids.size(0)\n",
        "\n",
        "            for b in range(batch_size):\n",
        "                if samples_processed >= num_samples:\n",
        "                    break\n",
        "\n",
        "                for split_ratio in [0.6, 0.7, 0.5, 0.8]:\n",
        "                    prompt_len = int(input_ids.size(1) * split_ratio)\n",
        "                    target = input_ids[b, prompt_len:]\n",
        "                    target_text = tokenizer.decode(target, skip_special_tokens=True).strip()\n",
        "\n",
        "                    if len(target_text) > 20:\n",
        "                        prompt = input_ids[b:b+1, :prompt_len]\n",
        "\n",
        "                        print(f\"\\n=== SAMPLE {samples_processed} (split={split_ratio}) ===\")\n",
        "                        prompt_text = tokenizer.decode(prompt[0], skip_special_tokens=True)\n",
        "                        print(f\"ðŸ“ PROMPT:\\n{prompt_text[-100:]}\")\n",
        "                        print(f\"ðŸŽ¯ TARGET:\\n{target_text[:100]}\")\n",
        "\n",
        "                        # FIXED: Pass eos_token_id for Pure GPT-2 compatibility\n",
        "                        generated = model.generate(\n",
        "                            prompt,\n",
        "                            max_length=prompt_len + 50,\n",
        "                            temperature=0.7,\n",
        "                            do_sample=True,\n",
        "                            top_k=50,\n",
        "                            top_p=0.9,\n",
        "                            repetition_penalty=1.2,\n",
        "                            eos_token_id=tokenizer.eos_token_id  # FIXED: Pass this parameter\n",
        "                        )\n",
        "                        generated_text = tokenizer.decode(generated[0, prompt_len:], skip_special_tokens=True)\n",
        "                        print(f\"ðŸ¤– GENERATED:\\n{generated_text[:100]}\")\n",
        "\n",
        "                        # Calculate BLEU\n",
        "                        reference = [target_text.split()]\n",
        "                        candidate = generated_text.split()\n",
        "\n",
        "                        if len(candidate) > 0:\n",
        "                            smoothing = SmoothingFunction().method1\n",
        "                            bleu = sentence_bleu(reference, candidate, smoothing_function=smoothing)\n",
        "                            bleu_scores.append(bleu)\n",
        "                            print(f\"ðŸ“Š BLEU: {bleu:.3f}\")\n",
        "\n",
        "                        samples_processed += 1\n",
        "                        break\n",
        "\n",
        "    return {\n",
        "        'bleu_score': sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0,\n",
        "        'exact_match': exact_matches / len(bleu_scores) if bleu_scores else 0,\n",
        "        'samples_evaluated': len(bleu_scores)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_syntax_validity(model, test_loader, tokenizer, device, num_samples=100):\n",
        "    \"\"\"Evaluate syntax validity of generated code\"\"\"\n",
        "    model.eval()\n",
        "    valid_codes = 0\n",
        "    total_codes = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input_ids, _) in enumerate(test_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Generate completions\n",
        "            generated = model.generate(\n",
        "                input_ids[:, :int(input_ids.size(1) * 0.5)],\n",
        "                max_length=input_ids.size(1), # This will ensure max_length is respected\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.6,             # Slightly lower for more deterministic syntax\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.2,\n",
        "                # No pad_token_id here as we handle stopping internally\n",
        "            )\n",
        "\n",
        "            for gen in generated:\n",
        "                code_text = tokenizer.decode(gen, skip_special_tokens=True)\n",
        "                # Extract just the code part (remove comments)\n",
        "                lines = code_text.split('\\n')\n",
        "                code_lines = [line for line in lines if not line.strip().startswith('#')]\n",
        "                code_only = '\\n'.join(code_lines)\n",
        "\n",
        "                if check_syntax_validity(code_only):\n",
        "                    valid_codes += 1\n",
        "                total_codes += 1\n",
        "\n",
        "    return valid_codes / total_codes if total_codes > 0 else 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_function_completion(model, test_loader, tokenizer, device):\n",
        "    \"\"\"Evaluate how well model completes function definitions\"\"\"\n",
        "    model.eval()\n",
        "    completion_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in test_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Find function definition start\n",
        "            text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "            if 'def ' in text:\n",
        "                def_pos = text.find('def ')\n",
        "                func_line_end = text.find('\\n', def_pos)\n",
        "\n",
        "                if func_line_end > 0:\n",
        "                    # Prompt is function signature + docstring\n",
        "                    prompt_text = text[:func_line_end + 1]\n",
        "                    target_text = text[func_line_end + 1:]\n",
        "\n",
        "                    # Tokenize prompt\n",
        "                    prompt_tokens = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "                    # Generate completion\n",
        "                    generated = model.generate(\n",
        "                        prompt_tokens,\n",
        "                        max_length=prompt_tokens.size(1) + 100, # Allow more length for function body\n",
        "                        num_return_sequences=1,\n",
        "                        temperature=0.5,             # Even lower for more focused completion\n",
        "                        do_sample=True,\n",
        "                        top_k=50,\n",
        "                        top_p=0.9,\n",
        "                        repetition_penalty=1.2,\n",
        "                        # No pad_token_id here as we handle stopping internally\n",
        "                    )\n",
        "\n",
        "                    # Evaluate completion quality\n",
        "                    generated_text = tokenizer.decode(\n",
        "                        generated[0, prompt_tokens.size(1):],\n",
        "                        skip_special_tokens=True\n",
        "                    )\n",
        "\n",
        "                    # Simple heuristics for function completion quality\n",
        "                    score = 0\n",
        "                    if 'return' in generated_text: score += 0.3\n",
        "                    if generated_text.count('    ') >= 1: score += 0.3  # Proper indentation\n",
        "                    if not any(char in generated_text for char in ['[UNK]', '<unk>']): score += 0.2\n",
        "                    if check_syntax_validity(prompt_text + generated_text): score += 0.2\n",
        "\n",
        "                    completion_scores.append(score)\n",
        "\n",
        "    return sum(completion_scores) / len(completion_scores) if completion_scores else 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_comprehensive(model, test_loader, device, tokenizer):\n",
        "    \"\"\"Comprehensive evaluation including perplexity and code-specific metrics\"\"\"\n",
        "\n",
        "    # Original perplexity\n",
        "    perplexity = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    print(f\"ðŸ” Running comprehensive code evaluation...\")\n",
        "\n",
        "    # Code generation quality\n",
        "    generation_metrics = evaluate_code_generation(model, test_loader, tokenizer, device)\n",
        "\n",
        "    # Syntax validity\n",
        "    syntax_validity = evaluate_syntax_validity(model, test_loader, tokenizer, device)\n",
        "\n",
        "    # Function completion\n",
        "    function_completion = evaluate_function_completion(model, test_loader, tokenizer, device)\n",
        "\n",
        "    return {\n",
        "        'perplexity': perplexity,\n",
        "        'bleu_score': generation_metrics['bleu_score'],\n",
        "        'exact_match': generation_metrics['exact_match'],\n",
        "        'syntax_validity': syntax_validity,\n",
        "        'function_completion': function_completion\n",
        "    }"
      ],
      "metadata": {
        "id": "pt9uXj9quilU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Section 3: Data Loading & Preprocessing #\n",
        " This section handles the MBPP (Mostly Basic Python Programs) dataset:\n",
        "- **Primary**: CodeSearchNet Python dataset (4,000 train + 800 test examples)\n",
        "- **Fallback**: High-quality synthetic Python code patterns\n",
        "- **Format**: Task description + code solution pairs\n",
        "- **Preprocessing**: Tokenization with GPT-2 tokenizer"
      ],
      "metadata": {
        "id": "fZrZa1uUytoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to import datasets, but don't fail if it's not available\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    DATASETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DATASETS_AVAILABLE = False\n",
        "    print(\"Datasets library not available, will use synthetic data\")\n",
        "\n",
        "\n",
        "\n",
        "def create_mbpp_data_loaders(tokenizer, device, max_length=512, batch_size=4):\n",
        "    \"\"\"Create data loaders for MBPP dataset or synthetic code data\"\"\"\n",
        "\n",
        "    class CodeDataset(Dataset):\n",
        "        def __init__(self, split='train', max_length=512):\n",
        "            self.examples = []\n",
        "            data_loaded = False\n",
        "\n",
        "            # Approach 1: Try loading CodeSearchNet Python (now with correct field names!)\n",
        "            if DATASETS_AVAILABLE and not data_loaded:\n",
        "                try:\n",
        "                    print(f\"Attempting to load CodeSearchNet Python {split} split...\")\n",
        "                    dataset = load_dataset('code_search_net', 'python', split=split, trust_remote_code=True)\n",
        "\n",
        "                    count = 0\n",
        "                    max_examples = 4000 if split == 'train' else 800  # Double the data\n",
        "\n",
        "                    for item in tqdm(dataset, desc=f\"Processing CodeSearchNet {split} data\"):\n",
        "                        if count >= max_examples:\n",
        "                            break\n",
        "\n",
        "                        # Use the CORRECT field names from the debug output\n",
        "                        code = item.get('func_code_string', '').strip()\n",
        "                        func_name = item.get('func_name', '').strip()\n",
        "                        docstring = item.get('func_documentation_string', '').strip()\n",
        "\n",
        "                        # Basic filtering for valid Python functions\n",
        "                        if code and len(code) > 30 and 'def ' in code:\n",
        "                            # Create a task-like format\n",
        "                            if docstring and len(docstring) > 5:\n",
        "                                text = f\"# Task: {docstring}\\n\\n# Solution:\\n{code}\"\n",
        "                            else:\n",
        "                                text = f\"# Function: {func_name}\\n\\n# Solution:\\n{code}\"\n",
        "\n",
        "                            # Tokenize\n",
        "                            tokens = tokenizer(\n",
        "                                text,\n",
        "                                truncation=True,\n",
        "                                max_length=max_length,\n",
        "                                padding='max_length',\n",
        "                                return_tensors='pt'\n",
        "                            )\n",
        "\n",
        "                            self.examples.append({\n",
        "                                'input_ids': tokens['input_ids'].squeeze(0),\n",
        "                                'labels': tokens['input_ids'].squeeze(0).clone()\n",
        "                            })\n",
        "                            count += 1\n",
        "\n",
        "                    if len(self.examples) > 0:\n",
        "                        data_loaded = True\n",
        "                        print(f\"Successfully loaded {len(self.examples)} CodeSearchNet examples for {split}\")\n",
        "                    else:\n",
        "                        print(f\"No valid examples found in CodeSearchNet {split}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load CodeSearchNet: {e}\")\n",
        "\n",
        "            # Approach 2: Create high-quality synthetic Python code data\n",
        "\n",
        "\n",
        "            ###\n",
        "            if not data_loaded:\n",
        "                print(f\"Creating synthetic Python code data for {split}...\")\n",
        "                num_examples = 500 if split == 'train' else 100\n",
        "\n",
        "                # More diverse and realistic code patterns\n",
        "                code_patterns = [\n",
        "                    # Function definitions\n",
        "                    \"def calculate_sum(numbers):\\n    '''Calculate sum of a list'''\\n    total = 0\\n    for num in numbers:\\n        total += num\\n    return total\",\n",
        "\n",
        "                    # Classes\n",
        "                    \"class DataProcessor:\\n    def __init__(self, data):\\n        self.data = data\\n        self.processed = False\\n    \\n    def process(self):\\n        self.data = [x * 2 for x in self.data]\\n        self.processed = True\",\n",
        "\n",
        "                    # List comprehensions and algorithms\n",
        "                    \"def find_primes(n):\\n    '''Find all prime numbers up to n'''\\n    primes = []\\n    for num in range(2, n + 1):\\n        is_prime = True\\n        for i in range(2, int(num ** 0.5) + 1):\\n            if num % i == 0:\\n                is_prime = False\\n                break\\n        if is_prime:\\n            primes.append(num)\\n    return primes\",\n",
        "\n",
        "                    # Recursion\n",
        "                    \"def fibonacci(n):\\n    '''Calculate nth Fibonacci number'''\\n    if n <= 1:\\n        return n\\n    return fibonacci(n - 1) + fibonacci(n - 2)\",\n",
        "\n",
        "                    # String manipulation\n",
        "                    \"def reverse_words(sentence):\\n    '''Reverse words in a sentence'''\\n    words = sentence.split()\\n    reversed_words = words[::-1]\\n    return ' '.join(reversed_words)\",\n",
        "\n",
        "                    # Dictionary operations\n",
        "                    \"def count_frequencies(items):\\n    '''Count frequency of each item'''\\n    freq_dict = {}\\n    for item in items:\\n        if item in freq_dict:\\n            freq_dict[item] += 1\\n        else:\\n            freq_dict[item] = 1\\n    return freq_dict\",\n",
        "\n",
        "                    # Error handling\n",
        "                    \"def safe_divide(a, b):\\n    '''Safely divide two numbers'''\\n    try:\\n        result = a / b\\n        return result\\n    except ZeroDivisionError:\\n        return None\\n    except TypeError:\\n        return 'Invalid input types'\",\n",
        "\n",
        "                    # Sorting algorithms\n",
        "                    \"def bubble_sort(arr):\\n    '''Implement bubble sort'''\\n    n = len(arr)\\n    for i in range(n):\\n        for j in range(0, n - i - 1):\\n            if arr[j] > arr[j + 1]:\\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\\n    return arr\",\n",
        "                ]\n",
        "\n",
        "                for i in range(num_examples):\n",
        "                    # Rotate through patterns and add variations\n",
        "                    base_code = code_patterns[i % len(code_patterns)]\n",
        "\n",
        "                    # Add task description\n",
        "                    task_descriptions = [\n",
        "                        \"Write a function to solve this problem\",\n",
        "                        \"Implement a solution for the following\",\n",
        "                        \"Create a Python function that handles this task\",\n",
        "                        \"Develop an algorithm to compute the result\",\n",
        "                    ]\n",
        "\n",
        "                    task = task_descriptions[i % len(task_descriptions)]\n",
        "                    full_text = f\"# Task: {task}\\n\\n{base_code}\"\n",
        "\n",
        "                    # Tokenize\n",
        "                    tokens = tokenizer(\n",
        "                        full_text,\n",
        "                        truncation=True,\n",
        "                        max_length=max_length,\n",
        "                        padding='max_length',\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "\n",
        "                    self.examples.append({\n",
        "                        'input_ids': tokens['input_ids'].squeeze(0),\n",
        "                        'labels': tokens['input_ids'].squeeze(0).clone()\n",
        "                    })\n",
        "\n",
        "                print(f\"Created {len(self.examples)} synthetic examples for {split}\")\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.examples)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.examples[idx]['input_ids'], self.examples[idx]['labels']\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\nPreparing datasets...\")\n",
        "    train_dataset = CodeDataset('train', max_length)\n",
        "    test_dataset = CodeDataset('test', max_length)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Avoid multiprocessing issues\n",
        "        pin_memory=True if device == 'cuda' else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True if device == 'cuda' else False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"   Training examples: {len(train_dataset)}\")\n",
        "    print(f\"   Test examples: {len(test_dataset)}\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   Max sequence length: {max_length}\")\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "3aM7aUrd6zft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Model Architectures"
      ],
      "metadata": {
        "id": "CTw1_1R27ZuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsection 4a: Pure GPT-2 Baseline\n",
        "##Pure GPT-2 Baseline Model\n",
        "\n",
        "Standard sequential transformer architecture:\n",
        "- **12-layer GPT-2** configuration\n",
        "- **Sequential processing** of code tokens\n",
        "- **No hierarchical structure**\n",
        "- **Baseline performance** for comparison"
      ],
      "metadata": {
        "id": "g2cSrXlU7eTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pure_gpt2_baseline(vocab_size):\n",
        "    \"\"\"Pure GPT-2 baseline for comparison\"\"\"\n",
        "\n",
        "    class PureGPT2(nn.Module):\n",
        "        def __init__(self, vocab_size, embed_dim=768):\n",
        "            super().__init__()\n",
        "\n",
        "            # Create GPT-2 configuration\n",
        "            self.config = GPT2Config(\n",
        "                vocab_size=vocab_size,\n",
        "                n_embd=embed_dim,\n",
        "                n_layer=12,\n",
        "                n_head=12,\n",
        "                activation_function='gelu_new',\n",
        "                resid_pdrop=0.1,\n",
        "                embd_pdrop=0.1,\n",
        "                attn_pdrop=0.1,\n",
        "            )\n",
        "\n",
        "            # Create GPT-2 model\n",
        "            self.gpt2 = GPT2Model(self.config)\n",
        "\n",
        "            # Output projection\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            # Forward through GPT-2\n",
        "            outputs = self.gpt2(input_ids)\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "\n",
        "            # Project to vocabulary\n",
        "            logits = self.output(hidden_states)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "            ##\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"FIXED: Use parameter instead of tokenizer reference\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "            eos_token_id = kwargs.get('eos_token_id', 50256)  # FIXED: Get EOS token from kwargs\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # FIXED: Use eos_token_id parameter instead of tokenizer\n",
        "                    if eos_token_id is not None and (next_token == eos_token_id).any():\n",
        "                        break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "\n",
        "    return PureGPT2(vocab_size)"
      ],
      "metadata": {
        "id": "BZQE3JXf7k2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsection 4b: Pure HNet Model\n",
        "##Pure HNet: End-to-End Hierarchical Architecture\n",
        "\n",
        "Novel hierarchical transformer with 4-level processing:\n",
        "1. **Chunk-Level Processing**: Fixed overlapping windows with attention pooling\n",
        "2. **Global Context**: Inter-chunk relationship modeling  \n",
        "3. **Hierarchical-Sequential Bridge**: Project back to sequence space\n",
        "4. **Final Sequence Processing**: Custom transformer layers\n",
        "\n",
        "**Key Innovations:**\n",
        "- Attention-based chunk pooling (vs mean pooling)\n",
        "- Explicit global context stage\n",
        "- End-to-end hierarchical optimization"
      ],
      "metadata": {
        "id": "BbiHLkkv74gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pure_hnet_model(vocab_size, tokenizer):\n",
        "    \"\"\"\n",
        "    Create Pure HNet model - End-to-end hierarchical transformer\n",
        "\n",
        "    Architecture Philosophy:\n",
        "    - Multi-level hierarchical processing (chunk â†’ global â†’ sequence)\n",
        "    - Attention-based chunk pooling for better representations\n",
        "    - Gated fusion between hierarchical and sequential features\n",
        "    - Designed specifically for structured text like code\n",
        "    \"\"\"\n",
        "\n",
        "    class PureHNetModel(nn.Module):\n",
        "        def __init__(self, vocab_size, tokenizer, embed_dim=768):\n",
        "            super().__init__()\n",
        "            self.tokenizer = tokenizer\n",
        "            self.embed_dim = embed_dim\n",
        "\n",
        "            print(f\"Building Pure HNet Architecture:\")\n",
        "            print(f\"Vocabulary: {vocab_size:,} tokens\")\n",
        "            print(f\"Embedding dimension: {embed_dim}\")\n",
        "\n",
        "            # ============= Core Embeddings =============\n",
        "            self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "            self.pos_embed = nn.Embedding(1024, embed_dim)\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "            # ============= Hierarchical Parameters =============\n",
        "            self.num_chunks = 32  # Number of hierarchical chunks\n",
        "            self.chunk_size = 512 // self.num_chunks\n",
        "            self.chunk_overlap = int(self.chunk_size * 0.25)  # 25% overlap\n",
        "\n",
        "            print(f\"Chunking strategy: {self.num_chunks} chunks of size {self.chunk_size}\")\n",
        "            print(f\"Chunk overlap: {self.chunk_overlap} tokens\")\n",
        "\n",
        "            # ============= Level 1: Chunk-Level Processing =============\n",
        "            chunk_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.chunk_encoder = nn.TransformerEncoder(chunk_encoder_layer, num_layers=6)\n",
        "\n",
        "            # ============= Level 2: Global Context Processing =============\n",
        "            global_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.global_encoder = nn.TransformerEncoder(global_encoder_layer, num_layers=4)\n",
        "\n",
        "            # ============= Level 3: Hierarchical-to-Sequential Bridge =============\n",
        "            self.chunk_projection = nn.Sequential(\n",
        "                nn.Linear(embed_dim, embed_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(embed_dim, embed_dim)\n",
        "            )\n",
        "\n",
        "            # Gating mechanism for hierarchical fusion\n",
        "            self.hierarchical_gate = nn.Sequential(\n",
        "                nn.Linear(embed_dim * 2, embed_dim),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            # ============= Level 4: Final Sequence Processing =============\n",
        "            sequence_encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.sequence_encoder = nn.TransformerEncoder(sequence_encoder_layer, num_layers=6)\n",
        "\n",
        "            # ============= Output Layer =============\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "            # Initialize weights\n",
        "            self.apply(self._init_weights)\n",
        "\n",
        "            total_params = sum(p.numel() for p in self.parameters())\n",
        "            print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "        def _init_weights(self, module):\n",
        "            \"\"\"Initialize model weights with small random values\"\"\"\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "        def _create_hierarchical_chunks(self, hidden_states, input_ids):\n",
        "            \"\"\"\n",
        "            Create hierarchical chunks with smart overlapping\n",
        "\n",
        "            Key Innovation: Attention-based pooling instead of mean pooling\n",
        "            for better chunk representations\n",
        "            \"\"\"\n",
        "            B, L, D = hidden_states.shape\n",
        "            chunks = []\n",
        "\n",
        "            for batch_idx in range(B):\n",
        "                batch_chunks = []\n",
        "\n",
        "                # Create overlapping chunks for better context\n",
        "                for i in range(self.num_chunks):\n",
        "                    start = i * self.chunk_size\n",
        "                    end = min(start + self.chunk_size + self.chunk_overlap, L)\n",
        "\n",
        "                    if start < L:\n",
        "                        chunk_tokens = hidden_states[batch_idx, start:end]\n",
        "                        # Use attention pooling for better representations\n",
        "                        chunk_repr = self._attention_pool(chunk_tokens)\n",
        "                        batch_chunks.append(chunk_repr)\n",
        "\n",
        "                # Ensure consistent chunk count\n",
        "                while len(batch_chunks) < self.num_chunks:\n",
        "                    batch_chunks.append(torch.zeros(D, device=hidden_states.device))\n",
        "\n",
        "                batch_chunks = batch_chunks[:self.num_chunks]\n",
        "                chunks.append(torch.stack(batch_chunks))\n",
        "\n",
        "            return torch.stack(chunks, dim=0)\n",
        "\n",
        "        def _attention_pool(self, chunk_tokens):\n",
        "            \"\"\"\n",
        "            Attention-based pooling for chunk representation\n",
        "\n",
        "            Better than mean pooling as it focuses on important tokens\n",
        "            \"\"\"\n",
        "            if chunk_tokens.size(0) == 0:\n",
        "                return torch.zeros(self.embed_dim, device=chunk_tokens.device)\n",
        "\n",
        "            # Compute attention weights based on token importance\n",
        "            chunk_mean = chunk_tokens.mean(dim=0, keepdim=True)\n",
        "            attention_scores = torch.sum(chunk_tokens * chunk_mean, dim=-1)\n",
        "            attention_weights = torch.softmax(attention_scores, dim=0)\n",
        "\n",
        "            # Weighted sum using attention\n",
        "            return torch.sum(chunk_tokens * attention_weights.unsqueeze(-1), dim=0)\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            \"\"\"\n",
        "            Four-level hierarchical forward pass:\n",
        "            1. Embeddings + Chunking\n",
        "            2. Chunk-level processing\n",
        "            3. Global context processing\n",
        "            4. Hierarchical-sequential fusion\n",
        "            5. Final sequence processing\n",
        "            \"\"\"\n",
        "            B, L = input_ids.shape\n",
        "\n",
        "            # ============= Level 1: Embeddings =============\n",
        "            positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            hidden_states = self.embed(input_ids) + self.pos_embed(positions)\n",
        "            hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "            # ============= Level 2: Chunk Processing =============\n",
        "            chunks = self._create_hierarchical_chunks(hidden_states, input_ids)\n",
        "            encoded_chunks = self.chunk_encoder(chunks)\n",
        "\n",
        "            # ============= Level 3: Global Context =============\n",
        "            global_context = self.global_encoder(encoded_chunks)\n",
        "\n",
        "            # ============= Level 4: Hierarchical-Sequential Bridge =============\n",
        "            # Project hierarchical features back to sequence space\n",
        "            projected_chunks = self.chunk_projection(global_context)\n",
        "\n",
        "            # Expand chunks back to sequence length\n",
        "            chunk_expanded = torch.repeat_interleave(projected_chunks, self.chunk_size, dim=1)\n",
        "            if chunk_expanded.size(1) > L:\n",
        "                chunk_expanded = chunk_expanded[:, :L, :]\n",
        "            elif chunk_expanded.size(1) < L:\n",
        "                padding = torch.zeros(B, L - chunk_expanded.size(1), self.embed_dim,\n",
        "                                    device=hidden_states.device)\n",
        "                chunk_expanded = torch.cat([chunk_expanded, padding], dim=1)\n",
        "\n",
        "            # Gated fusion between hierarchical and sequential features\n",
        "            gate_input = torch.cat([hidden_states, chunk_expanded], dim=-1)\n",
        "            gate = self.hierarchical_gate(gate_input)\n",
        "            combined = gate * chunk_expanded + (1 - gate) * hidden_states\n",
        "\n",
        "            # ============= Level 5: Final Sequence Processing =============\n",
        "            final_hidden = self.sequence_encoder(combined)\n",
        "\n",
        "            # Output projection\n",
        "            logits = self.output(final_hidden)\n",
        "\n",
        "            # Compute loss if labels provided\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"Robust generation method matching other models\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # Stopping conditions\n",
        "                    if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:\n",
        "                        if (next_token == self.tokenizer.eos_token_id).any():\n",
        "                            break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "    return PureHNetModel(vocab_size, tokenizer)"
      ],
      "metadata": {
        "id": "beCkNIoJ7_D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsection 4c: HNet-GPT2 Hybrid (Our Innovation)\n",
        "## HNet-GPT2 Hybrid: Our Novel Architecture\n",
        "\n",
        "**The main contribution**: Combines hierarchical encoding with proven GPT-2 generation:\n",
        "\n",
        "### Architecture Philosophy\n",
        "- **Hierarchical Encoder**: Code-aware adaptive chunking for structure understanding\n",
        "- **GPT-2 Decoder**: Proven autoregressive generation capabilities  \n",
        "- **Smart Fusion**: Complexity-aware gating and dynamic scaling\n",
        "\n",
        "### Key Innovations\n",
        "1. **Adaptive Chunking**: Code structure boundaries (def/class/if/for) vs fixed windows\n",
        "2. **Pre-trained Leverage**: Uses GPT-2 blocks instead of training from scratch\n",
        "3. **Gradual Training**: Strategic unfreezing for optimal learning\n",
        "4. **Complexity-Aware Integration**: Dynamic fusion based on code complexity\n",
        "\n",
        "### Why It Works\n",
        "- **Structure + Generation**: Gets hierarchical understanding AND proven generation\n",
        "- **Best of Both Worlds**: Combines strengths of both parent architectures\n",
        "- **Smart Training**: Leverages existing knowledge while adding structure awareness\n"
      ],
      "metadata": {
        "id": "urvxzbkq8XIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hnet_gpt2_hybrid(vocab_size, tokenizer):\n",
        "    \"\"\"HNet encoder with GPT-2 decoder blocks\"\"\"\n",
        "\n",
        "    class HNetGPT2Hybrid(nn.Module):\n",
        "        def __init__(self, vocab_size, tokenizer, embed_dim=768, num_chunks=24):\n",
        "            super().__init__()\n",
        "            self.tokenizer = tokenizer  # Store as instance variable\n",
        "\n",
        "            # Use GPT-2's embedding dimension for compatibility\n",
        "            self.embed_dim = embed_dim\n",
        "\n",
        "            # Embeddings (matching GPT-2's setup)\n",
        "            self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "            self.pos_embed = nn.Embedding(1024, embed_dim)\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "            # ============= HNet Encoder (from original) =============\n",
        "            self.num_chunks = 32  # Smaller chunks for better granularity\n",
        "            self.chunk_size = 512 // self.num_chunks\n",
        "            self.chunk_overlap = int(self.chunk_size * 0.25)  # 25% overlap\n",
        "\n",
        "\n",
        "            # Hierarchical chunk encoder (from HNet)\n",
        "            encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=12,  # Match GPT-2's attention heads\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.chunk_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "            # ============= GPT-2 Decoder Blocks =============\n",
        "            # Load GPT-2 configuration and extract decoder blocks\n",
        "            gpt2_config = GPT2Config(\n",
        "                vocab_size=vocab_size,\n",
        "                n_embd=embed_dim,\n",
        "                n_layer=12,  # Use 12 layers like GPT-2 small\n",
        "                n_head=12,\n",
        "                activation_function='gelu_new',\n",
        "                resid_pdrop=0.1,\n",
        "                embd_pdrop=0.1,\n",
        "                attn_pdrop=0.1,\n",
        "            )\n",
        "\n",
        "            # Create GPT-2 model and extract transformer blocks\n",
        "            gpt2_model = GPT2Model(gpt2_config)\n",
        "            self.gpt2_blocks = gpt2_model.h  # Extract the transformer blocks\n",
        "            self.ln_f = gpt2_model.ln_f  # Final layer norm\n",
        "\n",
        "\n",
        "            self.hierarchical_projection = nn.Sequential(\n",
        "                nn.Linear(embed_dim, embed_dim // 2),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(embed_dim // 2, embed_dim)\n",
        "            )\n",
        "\n",
        "            # Add dropout to gating:\n",
        "            self.hierarchical_gate = nn.Sequential(\n",
        "                nn.Linear(embed_dim * 2, embed_dim),\n",
        "                nn.Dropout(0.1),  # Add dropout here\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            # Initialize bias to favor original representations\n",
        "            nn.init.constant_(self.hierarchical_gate[0].bias, -2.0)\n",
        "\n",
        "\n",
        "            # Output projection\n",
        "            self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "            # Initialize weights\n",
        "            self.apply(self._init_weights)\n",
        "\n",
        "        def _init_weights(self, module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "\n",
        "        def _create_adaptive_chunks(self, hidden_states, input_ids):\n",
        "            \"\"\"Create chunks based on code structure boundaries\"\"\"\n",
        "            B, L, D = hidden_states.shape\n",
        "            chunks = []\n",
        "\n",
        "            for batch_idx in range(B):\n",
        "                # Decode tokens back to text to find structure\n",
        "                try:\n",
        "                    # Get non-padded tokens\n",
        "                    tokens = input_ids[batch_idx]\n",
        "                    text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "                    # Find function/class boundaries using simple heuristics\n",
        "                    boundaries = [0]\n",
        "                    lines = text.split('\\n')\n",
        "                    current_pos = 0\n",
        "\n",
        "                    for line in lines:\n",
        "                        if line.strip().startswith(('def ', 'class ', 'if ', 'for ', 'while ')):\n",
        "                            # Convert line position back to token position (approximation)\n",
        "                            char_pos = text.find(line)\n",
        "                            if char_pos > 0:\n",
        "                                # Rough token position estimate\n",
        "                                token_pos = min(int(char_pos * 0.3), L-1)  # Rough char-to-token ratio\n",
        "                                if token_pos > boundaries[-1] + 10:  # Minimum chunk size\n",
        "                                    boundaries.append(token_pos)\n",
        "\n",
        "                    boundaries.append(L)\n",
        "\n",
        "                except:\n",
        "                    # Fallback to fixed chunking if parsing fails\n",
        "                    boundaries = [i * (L // self.num_chunks) for i in range(self.num_chunks + 1)]\n",
        "                    boundaries[-1] = L\n",
        "\n",
        "                # Create chunks from boundaries\n",
        "                batch_chunks = []\n",
        "                for i in range(len(boundaries) - 1):\n",
        "                    start, end = boundaries[i], boundaries[i + 1]\n",
        "\n",
        "                    # Add overlap for non-first chunks\n",
        "                    if i > 0:\n",
        "                        start = max(0, start - self.chunk_overlap)\n",
        "\n",
        "                    # Add overlap for non-last chunks\n",
        "                    if i < len(boundaries) - 2:\n",
        "                        end = min(L, end + self.chunk_overlap)\n",
        "\n",
        "                    if end > start:\n",
        "                        chunk_tokens = hidden_states[batch_idx, start:end]\n",
        "                        chunk_repr = torch.mean(chunk_tokens, dim=0)\n",
        "                        batch_chunks.append(chunk_repr)\n",
        "\n",
        "                # Pad to consistent number of chunks\n",
        "                while len(batch_chunks) < self.num_chunks:\n",
        "                    batch_chunks.append(torch.zeros(D, device=hidden_states.device))\n",
        "\n",
        "                # Take first num_chunks if we have too many\n",
        "                batch_chunks = batch_chunks[:self.num_chunks]\n",
        "                chunks.append(torch.stack(batch_chunks))\n",
        "            #\n",
        "            return chunks  # chunks is already a list of tensors for each batch\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "\n",
        "        def forward(self, input_ids, labels=None):\n",
        "            B, L = input_ids.shape\n",
        "\n",
        "            # ============= Embeddings =============\n",
        "            positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            hidden_states = self.embed(input_ids) + self.pos_embed(positions)\n",
        "            hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "            # ============= HNet Hierarchical Encoding =============\n",
        "\n",
        "\n",
        "            # Create adaptive chunks based on code structure\n",
        "            chunks = self._create_adaptive_chunks(hidden_states, input_ids)\n",
        "            chunk_tensor = torch.stack(chunks, dim=0)  # dim=0 for batch dimension\n",
        "            encoded_chunks = self.chunk_encoder(chunk_tensor)\n",
        "\n",
        "\n",
        "            # ============= GPT-2 Decoder with Cross-Attention =============\n",
        "            # Before\n",
        "            # First, apply cross-attention to incorporate hierarchical info\n",
        "            # Now\n",
        "            # Expand chunk representations back to sequence length\n",
        "            chunk_expanded = torch.repeat_interleave(encoded_chunks, self.chunk_size, dim=1)\n",
        "            if chunk_expanded.size(1) > L:\n",
        "                chunk_expanded = chunk_expanded[:, :L, :]\n",
        "            elif chunk_expanded.size(1) < L:\n",
        "                padding = torch.zeros(B, L - chunk_expanded.size(1), self.embed_dim, device=hidden_states.device)\n",
        "                chunk_expanded = torch.cat([chunk_expanded, padding], dim=1)\n",
        "\n",
        "            # Project hierarchical features\n",
        "            hierarchical_features = self.hierarchical_projection(chunk_expanded)\n",
        "\n",
        "            # Learned gating\n",
        "            gate_input = torch.cat([hidden_states, hierarchical_features], dim=-1)\n",
        "            gate = self.hierarchical_gate(gate_input)\n",
        "\n",
        "            # With this (residual scaling):\n",
        "            gated_hierarchical = gate * hierarchical_features + (1 - gate) * hidden_states\n",
        "\n",
        "            complexity_score = torch.mean(gate, dim=-1, keepdim=True)  # [B, L, 1]\n",
        "            alpha = 0.8 + 0.2 * complexity_score  # Dynamic 0.8-1.0 range\n",
        "            hidden_states = alpha * hidden_states + (1 - alpha) * gated_hierarchical\n",
        "\n",
        "\n",
        "            # Apply GPT-2 transformer blocks\n",
        "            for block in self.gpt2_blocks:\n",
        "                outputs = block(hidden_states)\n",
        "                hidden_states = outputs[0]\n",
        "\n",
        "            # Final layer norm\n",
        "            hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "            # Output projection\n",
        "            logits = self.output(hidden_states)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "            ##\n",
        "        def generate(self, input_ids, max_length=None, num_return_sequences=1, temperature=1.0, do_sample=False, **kwargs):\n",
        "            \"\"\"FIXED: Properly reference self.tokenizer\"\"\"\n",
        "            self.eval()\n",
        "            device = input_ids.device\n",
        "\n",
        "            if max_length is None:\n",
        "                max_length = input_ids.size(1) + 50\n",
        "\n",
        "            max_length = min(max_length, 512)\n",
        "            generated = input_ids.clone()\n",
        "\n",
        "            top_k = kwargs.get('top_k', 50)\n",
        "            top_p = kwargs.get('top_p', 1.0)\n",
        "            repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(max_length - input_ids.size(1)):\n",
        "                    logits, _ = self.forward(generated)\n",
        "                    next_token_logits = logits[:, -1, :]\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if repetition_penalty != 1.0 and generated.size(1) > 0:\n",
        "                        for i in range(generated.size(0)):\n",
        "                            for prev_token_idx in set(generated[i, max(0, generated.size(1) - 10):].tolist()):\n",
        "                                if prev_token_idx < next_token_logits.size(-1):\n",
        "                                    if next_token_logits[i, prev_token_idx] < 0:\n",
        "                                        next_token_logits[i, prev_token_idx] *= repetition_penalty\n",
        "                                    else:\n",
        "                                        next_token_logits[i, prev_token_idx] /= repetition_penalty\n",
        "\n",
        "                    next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
        "\n",
        "                    if do_sample:\n",
        "                        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "                        all_filtered_out = (filtered_logits == -float('Inf')).all(dim=-1)\n",
        "                        if all_filtered_out.any():\n",
        "                            for i in range(all_filtered_out.size(0)):\n",
        "                                if all_filtered_out[i]:\n",
        "                                    filtered_logits[i, 0] = 1.0\n",
        "\n",
        "                        filtered_logits = torch.nan_to_num(filtered_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "                        probs = F.softmax(filtered_logits, dim=-1)\n",
        "                        probs = torch.clamp(probs, min=1e-9)\n",
        "                        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "                        next_token = torch.multinomial(probs, num_samples=1)\n",
        "                    else:\n",
        "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                    generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "                    # FIXED: Use self.tokenizer instead of tokenizer\n",
        "                    if self.tokenizer.eos_token_id is not None and (next_token == self.tokenizer.eos_token_id).any():\n",
        "                        break\n",
        "\n",
        "                    if generated.size(1) >= input_ids.size(1) + 3:\n",
        "                        if (generated[:, -1] == generated[:, -2]).any() and \\\n",
        "                          (generated[:, -1] == generated[:, -3]).any():\n",
        "                            break\n",
        "\n",
        "            return generated\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return HNetGPT2Hybrid(vocab_size, tokenizer)"
      ],
      "metadata": {
        "id": "fCE_IQyE8e1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Section 5: Training Infrastructure\n",
        " # Training Infrastructure\n",
        "\n",
        "Advanced training setup with:\n",
        "- **Gradual Unfreezing**: Strategic component unfreezing for hybrid model\n",
        "- **Warmup Scheduling**: Learning rate warmup + cosine decay\n",
        "- **Gradient Clipping**: Stable training for large models\n",
        "- **Model Checkpointing**: Automatic saving to Google Drive"
      ],
      "metadata": {
        "id": "qvCYBt5783mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, device, epochs=25, lr=1e-4, model_name=\"Model\"):\n",
        "    \"\"\"Train model with warmup + freezing strategy\"\"\"\n",
        "\n",
        "    # Phase 1: Freeze GPT-2 blocks\n",
        "    if hasattr(model, 'gpt2_blocks'):\n",
        "        for block in model.gpt2_blocks:\n",
        "            for param in block.parameters():\n",
        "                param.requires_grad = False\n",
        "        print(\"Phase 1: GPT-2 blocks frozen, training hierarchical components only\")\n",
        "\n",
        "    # Calculate warmup steps\n",
        "    warmup_epochs = 2\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = len(train_loader) * warmup_epochs\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=lr * 0.01,  # Start with very small LR\n",
        "        betas=(0.9, 0.95), weight_decay=0.05\n",
        "    )\n",
        "\n",
        "    # Warmup + Cosine scheduler\n",
        "    from transformers import get_linear_schedule_with_warmup\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Unfreezing schedule (same as before)\n",
        "        if hasattr(model, 'gpt2_blocks') and epoch == 8:\n",
        "            print(\"Phase 2: Unfreezing last 6 GPT-2 blocks\")\n",
        "            for block in model.gpt2_blocks[-6:]:\n",
        "                for param in block.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        elif hasattr(model, 'gpt2_blocks') and epoch == 16:\n",
        "            print(\"Phase 3: Unfreezing all GPT-2 blocks\")\n",
        "            for block in model.gpt2_blocks:\n",
        "                for param in block.parameters():\n",
        "                    param.requires_grad = True\n",
        "            # Update optimizer\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                lr=lr * 0.3,\n",
        "                betas=(0.9, 0.95), weight_decay=0.05\n",
        "            )\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_batches = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for input_ids, labels in pbar:\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits, loss = model(input_ids, labels=labels)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Step scheduler every batch during warmup\n",
        "                if step < total_steps:\n",
        "                    scheduler.step()\n",
        "                    step += 1\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_batches += 1\n",
        "\n",
        "                # Show current LR in progress bar\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'lr': f'{current_lr:.6f}'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Training error: {e}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = epoch_loss / max(epoch_batches, 1)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"   Epoch {epoch+1}: Loss = {avg_loss:.4f}, LR = {current_lr:.6f}\")\n",
        "\n",
        "    # Save model\n",
        "    save_path = f\"/content/drive/MyDrive/hnet_gpt2_models/{model_name.replace(' ', '_')}.pt\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Saved model to {save_path}\")\n",
        "\n",
        "    return {'losses': losses, 'final_loss': losses[-1] if losses else float('inf')}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate model on test data\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            try:\n",
        "                logits, loss = model(input_ids, labels=labels)\n",
        "\n",
        "                # Count tokens\n",
        "                mask = (labels != -100).float()\n",
        "                total_loss += loss.item() * mask.sum().item()\n",
        "                total_tokens += mask.sum().item()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Evaluation error: {e}\")\n",
        "                continue\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 100))  # Cap to avoid overflow\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_results(results):\n",
        "    \"\"\"Print benchmark results\"\"\"\n",
        "\n",
        "    print(f\"\\nHYBRID ARCHITECTURE RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Sort by perplexity\n",
        "    sorted_results = sorted([r for r in results if r['test_perplexity'] != float('inf')],\n",
        "                           key=lambda x: x['test_perplexity'])\n",
        "\n",
        "    print(f\"FINAL RANKING:\")\n",
        "    print(f\"{'Rank':<5} {'Model':<30} {'Perplexity':<12} {'Params':<10} {'Time(s)':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for i, result in enumerate(sorted_results, 1):\n",
        "        print(f\"{i:<5} {result['name']:<30} {result['test_perplexity']:<12.2f} \"\n",
        "              f\"{result['parameters']:>9,} {result['training_time']:<10.1f}\")\n",
        "\n",
        "    # Compare hybrid vs pure GPT-2\n",
        "    hybrid_result = next((r for r in results if 'Hybrid' in r['name']), None)\n",
        "    gpt2_result = next((r for r in results if 'Pure' in r['name']), None)\n",
        "\n",
        "    if hybrid_result and gpt2_result:\n",
        "        improvement = ((gpt2_result['test_perplexity'] - hybrid_result['test_perplexity']) /\n",
        "                      gpt2_result['test_perplexity']) * 100\n",
        "\n",
        "        print(f\"\\nHYBRID ARCHITECTURE ANALYSIS:\")\n",
        "        print(f\"HNet-GPT2-Hybrid:  {hybrid_result['test_perplexity']:.2f} perplexity\")\n",
        "        print(f\"Pure GPT-2:        {gpt2_result['test_perplexity']:.2f} perplexity\")\n",
        "        print(f\"Improvement:       {improvement:.1f}%\")\n",
        "\n",
        "        print(f\"\\nARCHITECTURAL INSIGHTS:\")\n",
        "        print(f\"â€¢ HNet hierarchical encoding: Provides chunk-level context\")\n",
        "        print(f\"â€¢ GPT-2 decoder blocks: Leverages pretrained architecture\")\n",
        "        print(f\"â€¢ Cross-attention fusion: Combines hierarchical + sequential\")\n",
        "        print(f\"â€¢ Real MBPP data: Tests on actual code understanding tasks\")"
      ],
      "metadata": {
        "id": "TklX1fgL88uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 6: Three-Way Architecture Comparison\n",
        "# Three-Way Architecture Comparison\n",
        "\n",
        "**The main experiment**: Comprehensive evaluation of all three approaches:\n",
        "\n",
        "## Research Question\n",
        "Which approach works best for code generation?\n",
        "1. **Pure hierarchical processing** (Pure HNet)\n",
        "2. **Pure sequential processing** (Pure GPT-2)  \n",
        "3. **Hybrid hierarchical-sequential** (Our HNet-GPT2)\n",
        "\n",
        "## Evaluation Metrics\n",
        "- **Perplexity**: Language modeling performance\n",
        "- **BLEU Score**: Generation quality vs ground truth\n",
        "- **Syntax Validity**: Percentage of syntactically correct code\n",
        "- **Function Completion**: Quality of function body completion\n",
        "\n",
        "## Experimental Setup\n",
        "- **Dataset**: MBPP (CodeSearchNet Python)\n",
        "- **Training**: 25 epochs with adaptive learning rate\n",
        "- **Hardware**: GPU acceleration with CUDA\n",
        "- **Reproducibility**: Fixed random seeds and comprehensive logging"
      ],
      "metadata": {
        "id": "7JKfkY879bwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_complete_three_way_comparison():\n",
        "    \"\"\"\n",
        "    Run comprehensive comparison of all three architectures:\n",
        "    1. Pure HNet (end-to-end hierarchical)\n",
        "    2. HNet-GPT2 Hybrid (hierarchical encoder + GPT-2 decoder)\n",
        "    3. Pure GPT-2 (sequential baseline)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nLAUNCHING COMPREHENSIVE THREE-WAY COMPARISON\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Load tokenizer and setup\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    print(f\"Vocabulary size: {vocab_size:,}\")\n",
        "\n",
        "    # Architecture configurations\n",
        "    models_config = [\n",
        "        {\n",
        "            'name': 'Pure HNet',\n",
        "            'model': create_pure_hnet_model(vocab_size, tokenizer),\n",
        "            'lr': 1e-4,\n",
        "            'description': 'End-to-end hierarchical transformer (4-level processing)',\n",
        "            'color': '#FF9500',\n",
        "            'innovation': 'Multi-level hierarchy with attention pooling'\n",
        "        },\n",
        "        {\n",
        "            'name': 'HNet-GPT2-Hybrid',\n",
        "            'model': create_hnet_gpt2_hybrid(vocab_size, tokenizer),\n",
        "            'lr': 1e-4,\n",
        "            'description': 'Hierarchical encoder + GPT-2 decoder blocks',\n",
        "            'color': '#FF6B6B',\n",
        "            'innovation': 'Best of both worlds: hierarchy + proven GPT-2'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Pure GPT-2',\n",
        "            'model': create_pure_gpt2_baseline(vocab_size),\n",
        "            'lr': 1e-4,\n",
        "            'description': 'Standard sequential transformer (baseline)',\n",
        "            'color': '#45B7D1',\n",
        "            'innovation': 'Proven architecture for autoregressive generation'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Load MBPP dataset\n",
        "    print(\"\\nLoading MBPP dataset for code generation evaluation...\")\n",
        "    train_loader, test_loader = create_mbpp_data_loaders(tokenizer, device)\n",
        "\n",
        "    print(f\"Dataset loaded:\")\n",
        "    print(f\"   Training batches: {len(train_loader)}\")\n",
        "    print(f\"   Test batches: {len(test_loader)}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for i, config in enumerate(models_config, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"MODEL {i}/3: {config['name']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\" {config['description']}\")\n",
        "        print(f\" Innovation: {config['innovation']}\")\n",
        "        print(f\" Learning rate: {config['lr']}\")\n",
        "\n",
        "        model = config['model'].to(device)\n",
        "        params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"Parameters: {params:,}\")\n",
        "\n",
        "        # Check if model already exists\n",
        "        model_path = f\"/content/drive/MyDrive/hnet_gpt2_models/{config['name'].replace(' ', '_')}.pt\"\n",
        "        model_exists = os.path.exists(model_path)\n",
        "\n",
        "        if model_exists:\n",
        "            print(f\"Found existing model, loading from {model_path}\")\n",
        "            try:\n",
        "                model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "                print(f\"Model loaded successfully\")\n",
        "                training_time = 0  # No training needed\n",
        "                final_loss = 0.0  # Unknown for loaded model\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load model: {e}\")\n",
        "                print(f\"Training new model instead...\")\n",
        "                model_exists = False\n",
        "\n",
        "        if not model_exists:\n",
        "            # Training\n",
        "            print(f\"\\nTRAINING {config['name']}...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            training_history = train_model(\n",
        "                model, train_loader, device,\n",
        "                epochs=25,\n",
        "                lr=config['lr'],\n",
        "                model_name=config['name']\n",
        "            )\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "            final_loss = training_history['final_loss']\n",
        "\n",
        "            print(f\"Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "        # Comprehensive Evaluation\n",
        "        print(f\"\\nEVALUATING {config['name']}...\")\n",
        "\n",
        "        # 1. Perplexity evaluation\n",
        "        print(\"Computing perplexity...\")\n",
        "        test_perplexity = evaluate_model(model, test_loader, device)\n",
        "\n",
        "        # 2. Code generation evaluation\n",
        "        print(\"Testing code generation...\")\n",
        "        generation_metrics = evaluate_code_generation(model, test_loader, tokenizer, device, num_samples=20)\n",
        "\n",
        "        # 3. Syntax validity\n",
        "        print(\"Checking syntax validity...\")\n",
        "        syntax_validity = evaluate_syntax_validity(model, test_loader, tokenizer, device, num_samples=50)\n",
        "\n",
        "        # 4. Function completion\n",
        "        print(\"Testing function completion...\")\n",
        "        function_completion = evaluate_function_completion(model, test_loader, tokenizer, device)\n",
        "\n",
        "        # Store comprehensive results\n",
        "        result = {\n",
        "            'name': config['name'],\n",
        "            'description': config['description'],\n",
        "            'innovation': config['innovation'],\n",
        "            'test_perplexity': test_perplexity,\n",
        "            'training_time': training_time,\n",
        "            'parameters': params,\n",
        "            'final_loss': final_loss,\n",
        "            'bleu_score': generation_metrics['bleu_score'],\n",
        "            'exact_match': generation_metrics['exact_match'],\n",
        "            'syntax_validity': syntax_validity,\n",
        "            'function_completion': function_completion,\n",
        "            'color': config['color']\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Print individual results\n",
        "        print(f\"\\n{config['name']} RESULTS:\")\n",
        "        print(f\"   Test Perplexity: {test_perplexity:.2f}\")\n",
        "        print(f\"   BLEU Score: {generation_metrics['bleu_score']:.3f}\")\n",
        "        print(f\"   Exact Match: {generation_metrics['exact_match']:.3f}\")\n",
        "        print(f\"   Syntax Validity: {syntax_validity:.3f}\")\n",
        "        print(f\"   Function Completion: {function_completion:.3f}\")\n",
        "        print(f\"   Training Time: {training_time:.1f}s\")\n",
        "\n",
        "    # Comprehensive analysis\n",
        "    print_comprehensive_three_way_analysis(results)\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_comprehensive_three_way_analysis(results):\n",
        "    \"\"\"Print detailed three-way analysis with insights\"\"\"\n",
        "\n",
        "    print(f\"\\nCOMPREHENSIVE THREE-WAY ARCHITECTURE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Overall ranking table\n",
        "    print(f\"\\nOVERALL PERFORMANCE RANKING:\")\n",
        "    print(f\"{'Rank':<5} {'Architecture':<20} {'Perplexity':<12} {'BLEU':<8} {'Syntax':<8} {'Completion':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Sort by perplexity (primary metric)\n",
        "    sorted_results = sorted(results, key=lambda x: x['test_perplexity'])\n",
        "\n",
        "    for i, result in enumerate(sorted_results, 1):\n",
        "        emoji = \"ðŸ¥‡\" if i == 1 else \"ðŸ¥ˆ\" if i == 2 else \"ðŸ¥‰\"\n",
        "        print(f\"{emoji} {i:<3} {result['name']:<20} {result['test_perplexity']:<12.2f} \"\n",
        "              f\"{result['bleu_score']:<8.3f} {result['syntax_validity']:<8.3f} {result['function_completion']:<12.3f}\")\n",
        "\n",
        "    # Detailed architecture comparison\n",
        "    print(f\"\\nðŸ§  ARCHITECTURAL INSIGHTS:\")\n",
        "\n",
        "    # Find each model\n",
        "    pure_hnet = next((r for r in results if 'Pure HNet' in r['name']), None)\n",
        "    hybrid = next((r for r in results if 'Hybrid' in r['name']), None)\n",
        "    gpt2 = next((r for r in results if 'Pure GPT-2' in r['name']), None)\n",
        "\n",
        "    if all([pure_hnet, hybrid, gpt2]):\n",
        "        # Compare perplexities\n",
        "        print(f\"\\nPERPLEXITY COMPARISON:\")\n",
        "        print(f\"   Pure HNet:         {pure_hnet['test_perplexity']:.2f}\")\n",
        "        print(f\"   HNet-GPT2-Hybrid: {hybrid['test_perplexity']:.2f}\")\n",
        "        print(f\"   Pure GPT-2:       {gpt2['test_perplexity']:.2f}\")\n",
        "\n",
        "        # Calculate improvements\n",
        "        improvements = {}\n",
        "        if hybrid['test_perplexity'] < gpt2['test_perplexity']:\n",
        "            improvements['hybrid_vs_gpt2'] = ((gpt2['test_perplexity'] - hybrid['test_perplexity']) / gpt2['test_perplexity']) * 100\n",
        "\n",
        "        if hybrid['test_perplexity'] < pure_hnet['test_perplexity']:\n",
        "            improvements['hybrid_vs_hnet'] = ((pure_hnet['test_perplexity'] - hybrid['test_perplexity']) / pure_hnet['test_perplexity']) * 100\n",
        "\n",
        "        if pure_hnet['test_perplexity'] < gpt2['test_perplexity']:\n",
        "            improvements['hnet_vs_gpt2'] = ((gpt2['test_perplexity'] - pure_hnet['test_perplexity']) / gpt2['test_perplexity']) * 100\n",
        "\n",
        "        print(f\"\\nðŸ“Š IMPROVEMENT ANALYSIS:\")\n",
        "        for comparison, improvement in improvements.items():\n",
        "            comparison_name = comparison.replace('_', ' ').replace('vs', 'vs.').title()\n",
        "            print(f\"   ðŸ“ˆ {comparison_name}: {improvement:+.1f}%\")\n",
        "\n",
        "        # Determine winner and insights\n",
        "        winner = sorted_results[0]\n",
        "        print(f\"\\nWINNER: {winner['name']}\")\n",
        "        print(f\"Best Perplexity: {winner['test_perplexity']:.2f}\")\n",
        "\n",
        "        # Architecture-specific insights\n",
        "        print(f\"\\nðŸ” ARCHITECTURE-SPECIFIC INSIGHTS:\")\n",
        "\n",
        "        if winner['name'] == 'Pure HNet':\n",
        "            print(f\"   END-TO-END HIERARCHY WINS!\")\n",
        "            print(f\"   Full hierarchical processing is superior for code\")\n",
        "            print(f\"   Multi-level attention and chunk processing pays off\")\n",
        "            print(f\"   Hierarchical structure naturally fits code's nested nature\")\n",
        "\n",
        "        elif winner['name'] == 'HNet-GPT2-Hybrid':\n",
        "            print(f\"   HYBRID APPROACH WINS!\")\n",
        "            print(f\"   Best of both worlds: hierarchy + proven GPT-2\")\n",
        "            print(f\"   Hierarchical encoding with sequential generation\")\n",
        "            print(f\"   Smart combination outperforms individual approaches\")\n",
        "\n",
        "        else:  # Pure GPT-2\n",
        "            print(f\"   SEQUENTIAL PROCESSING WINS!\")\n",
        "            print(f\"   Sometimes simpler is better\")\n",
        "            print(f\"   Proven GPT-2 architecture still dominates\")\n",
        "            print(f\"   Sequential modeling sufficient for this task\")\n",
        "\n",
        "        # Code-specific metric analysis\n",
        "        print(f\"\\nCODE GENERATION ANALYSIS:\")\n",
        "\n",
        "        best_bleu = max(results, key=lambda x: x['bleu_score'])\n",
        "        best_syntax = max(results, key=lambda x: x['syntax_validity'])\n",
        "        best_completion = max(results, key=lambda x: x['function_completion'])\n",
        "\n",
        "        print(f\"   Best BLEU Score: {best_bleu['name']} ({best_bleu['bleu_score']:.3f})\")\n",
        "        print(f\"   Best Syntax: {best_syntax['name']} ({best_syntax['syntax_validity']:.3f})\")\n",
        "        print(f\"   Best Completion: {best_completion['name']} ({best_completion['function_completion']:.3f})\")\n",
        "\n",
        "        # Parameter efficiency\n",
        "        print(f\"\\nPARAMETER EFFICIENCY:\")\n",
        "        for result in sorted(results, key=lambda x: x['parameters']):\n",
        "            efficiency = result['parameters'] / (1 / result['test_perplexity'])  # Lower is better\n",
        "            print(f\"   {result['name']}: {result['parameters']:,} params, \"\n",
        "                  f\"efficiency: {efficiency:.0f}\")"
      ],
      "metadata": {
        "id": "9_POKeL69tv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 7: Quick Execution & Results\n",
        "# Quick Execution & Results\n",
        "\n",
        "Run this section to reproduce our main results:\n",
        "\n",
        "## One-Click Execution"
      ],
      "metadata": {
        "id": "geX4h4Ml-hc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick execution for reproducing results\n",
        "print(\"REPRODUCING HNET-GPT RESULTS\")\n",
        "print(\"=\" * 40)\n",
        "print(\"This will run the complete three-way comparison...\")\n",
        "print(\"Expected runtime: ~3-4 hours on Tesla T4\")\n",
        "print()\n",
        "\n",
        "# Run the complete comparison\n",
        "results = run_complete_three_way_comparison()\n",
        "\n",
        "# Display final summary\n",
        "print(\"\\nEXPERIMENT COMPLETE!\")\n",
        "print(\"Your results should show HNet-GPT2-Hybrid winning with ~40% improvement!\")"
      ],
      "metadata": {
        "id": "A_sdFYiZ-mPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 8: Results Analysis & Visualization (Optional)\n",
        "# Results Analysis & Visualization\n",
        "\n",
        "Deep dive into the experimental results:\n",
        "- **Performance comparison charts**\n",
        "- **Training loss curves**\n",
        "- **Generated code examples**\n",
        "- **Statistical significance testing**"
      ],
      "metadata": {
        "id": "jc0ntKtU-uYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Results visualization and analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_comparison_results(results):\n",
        "    # Create comparison charts\n",
        "    # Plot training curves\n",
        "    # Show example generations\n",
        "    pass\n",
        "\n",
        "def analyze_statistical_significance(results):\n",
        "    # Statistical analysis of results\n",
        "    pass"
      ],
      "metadata": {
        "id": "_uPyK1xq-08D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "kyJZLyap_TJs",
        "hOVo5BEVsjqr",
        "Y5k70HaFuZUQ",
        "fZrZa1uUytoz",
        "g2cSrXlU7eTK",
        "BbiHLkkv74gF",
        "urvxzbkq8XIJ",
        "qvCYBt5783mj",
        "7JKfkY879bwo"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}